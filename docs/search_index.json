[["index.html", "R cookbook for the casual dabbler Chapter 1 Introduction 1.1 Usage 1.2 Additional resources 1.3 Limitations 1.4 About the author", " R cookbook for the casual dabbler Charles Coverdale 2021-10-29 Chapter 1 Introduction G’day and welcome to R cookbook for the casual dabbler. Some history: I use R a lot for work and for side projects. Over the years I’ve collated a bunch of useful scripts, from macroeconomic analysis to quick hacks for making map legends format properly. Historically my code has been stored in random Rpubs documents, medium articles, and a bunch of .Rmd files on my hardrive. Occasionally I feel like doing things properly - and upload code to a repository on github. It doesn’t take a genius to realize this isn’t a very sustainable solution - and it also isn’t very useful for sharing code with others. It turns out 2-years of lockdown in Melbourne was enough incentive to sit down and collate my best and most useful code into a single place. In the spirit of open source, a book seemed like the most logical format. The following is a very rough book written in markdown - R’s very own publishing language. 1.1 Usage In each chapter I’ve written up the background, methodology and code for a separate piece of analysis. Most of this code will not be extraordinary to the seasoned R aficionado. The vast majority can be found elsewhere if you dig around on stackexchange or read some of Hadley’s books. However I find that in classic Pareto style ~20% of my code contributes to the vast majority of my work output. Having this on hand will hopefully be useful to both myself and others. 1.2 Additional resources The R community is continually writing new books and package documentation with great worked examples. Some of my favourites (which all happen to be written in the R markdown language) are: Geocomputation with R R Markdown: The Definite Guide R Cookbook, 2nd Edition R for Data Science Data Science in Education using R Introduction to R: Walter and Eliza Hall Institute PhD lectures notes in environmental economics and data science (University of Oregon) 1.3 Limitations I’ll be honest with you - there’s bound to be bugs galore in this. If you find one (along with spelling errors etc) please email me at charlesfcoverdale@gmail.com with the subject line ‘R cookbook for the casual dabbler.’ 1.4 About the author Charles Coverdale is an economist based in Melbourne, Australia. He is passionate about economics, climate science, and building talented teams. You can get in touch with Charles on twitter to hear more about his current projects. "],["basic-modelling-in-r.html", "Chapter 2 Basic modelling in R 2.1 Source, format, and plot data 2.2 Build a linear model 2.3 Analyse the model fit 2.4 Compare the predicted values with the actual values 2.5 Analyse the residuals 2.6 Linear regression with more than one variable 2.7 Fitting a polynomial regression 2.8 —", " Chapter 2 Basic modelling in R Creating a model is an essential part of forecasting and data analysis. I’ve put together a quick guide on my process for modelling data and checking model fit. The source data I use in this example is Melbourne’s weather record over a 12 month period. Daily temperature is based on macroscale weather and climate systems, however many observable measurements are correlated (i.e. hot days tend to have lots of sunshine). This makes using weather data great for model building. 2.1 Source, format, and plot data Before we get started, it is useful to have some packages up and running. #Useful packages for regression library(readr) library(readxl) library(ggplot2) library(dplyr) library(tidyverse) library(lubridate) library(modelr) library(cowplot) I’ve put together a csv file of weather observations in Melbourne in 2019. We begin our model by downloading the data from Github. #Input data url &lt;-&quot;https://raw.githubusercontent.com/charlescoverdale/predicttemperature/master/MEL_weather_2019.csv&quot; #We&#39;ll read this data in as a dataframe. The &#39;check.names&#39; function set to false means the funny units that the BOM use for column names won&#39;t affect the import. MEL_weather_2019 &lt;- read.csv(url, check.names = F) head(MEL_weather_2019) This data is relatively clean. One handy change to make is to make the date into a dynamic format (to easily switch between months, years, etc). #Add a proper date column MEL_weather_2019 &lt;- MEL_weather_2019 %&gt;% mutate(Date = make_date(Year, Month, Day)) We also notice that some of the column names have symbols in them. This can be tricky to work with, so let’s rename some columns into something more manageable. #Rename key df variables names(MEL_weather_2019)[4]&lt;- &quot;Solar_exposure&quot; names(MEL_weather_2019)[5]&lt;- &quot;Rainfall&quot; names(MEL_weather_2019)[6]&lt;- &quot;Max_temp&quot; head(MEL_weather_2019) We’re aiming to investigate if other weather variables can predict maximum temperatures. Solar exposure seems like a plausible place to start. We start by plotting the two variables to if there is a trend. #Plot the data MEL_temp_investigate &lt;- ggplot(MEL_weather_2019)+ geom_point(aes(y=Max_temp, x=Solar_exposure),col=&quot;grey&quot;)+ labs(title = &quot;Does solar exposure drive temperature in Melbourne?&quot;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlab(&quot;Solar exposure&quot;)+ ylab(&quot;Maximum temperature °C&quot;)+ scale_x_continuous(expand=c(0,0))+ theme_bw()+ theme(axis.text=element_text(size=10))+ theme(panel.grid.minor = element_blank()) MEL_temp_investigate Eyeballing the chart above, there seems to be a correlation between the two data sets. We’ll do one more quick plot to analyse the data. What is the distribution of temperature? ggplot(MEL_weather_2019, aes(x=Max_temp)) + geom_histogram(aes(y=..density..), colour=&quot;black&quot;, fill=&quot;lightblue&quot;)+ geom_density(alpha=.5, fill=&quot;grey&quot;,colour=&quot;darkblue&quot;)+ scale_x_continuous(breaks=c(5,10,15,20,25,30,35,40,45), expand=c(0,0))+ xlab(&quot;Temperature&quot;)+ ylab(&quot;Density&quot;)+ theme_bw()+ theme(axis.text=element_text(size=12))+ theme(panel.grid.minor = element_blank()) We can see here the data is right skewed (i.e. the mean will be greater than the median). We’ll need to keep this in mind. Let’s start building a model. 2.2 Build a linear model We start by looking whether a simple linear regression of solar exposure seems to be correlated with temperature. In R, we can use the linear model (lm) function. #Create a straight line estimate to fit the data temp_model &lt;- lm(Max_temp~Solar_exposure, data=MEL_weather_2019) 2.3 Analyse the model fit Let’s see how well solar exposure explains changes in temperature #Call a summary of the model summary(temp_model) The adjusted R squared value (one measure of model fit) is 0.3596. Furthermore the coefficient of our solar_exposure variable is statistically significant. 2.4 Compare the predicted values with the actual values We can use this lm function to predict values of temperature based on the level of solar exposure. We can then compare this to the actual temperature record, and see how well the model fits the data set. #Use this lm model to predict the values MEL_weather_2019 &lt;- MEL_weather_2019 %&gt;% mutate(predicted_temp=predict(temp_model,newdata=MEL_weather_2019)) #Calculate the prediction interval prediction_interval &lt;- predict(temp_model, newdata=MEL_weather_2019, interval = &quot;prediction&quot;) summary(prediction_interval) #Bind this prediction interval data back to the main set MEL_weather_2019 &lt;- cbind(MEL_weather_2019,prediction_interval) MEL_weather_2019 Model fit is easier to interpret graphically. Let’s plot the data with the model overlaid. #Plot a chart with data and model on it MEL_temp_predicted &lt;- ggplot(MEL_weather_2019)+ geom_point(aes(y=Max_temp, x=Solar_exposure), col=&quot;grey&quot;)+ geom_line(aes(y=predicted_temp,x=Solar_exposure), col=&quot;blue&quot;)+ geom_smooth(aes(y=Max_temp, x= Solar_exposure), method=lm)+ geom_line(aes(y=lwr,x=Solar_exposure), colour=&quot;red&quot;, linetype=&quot;dashed&quot;)+ geom_line(aes(y=upr,x=Solar_exposure), colour=&quot;red&quot;, linetype=&quot;dashed&quot;)+ labs(title = &quot;Does solar exposure drive temperature in Melbourne?&quot;, subtitle = &#39;Investigation using linear regression&#39;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlab(&quot;Solar exposure&quot;)+ ylab(&quot;Maximum temperature °C&quot;)+ scale_x_continuous(expand=c(0,0), breaks=c(0,5,10,15,20,25,30,35,40))+ theme_bw()+ theme(axis.text=element_text(size=10))+ theme(panel.grid.minor = element_blank()) MEL_temp_predicted This chart includes the model (blue line), confidence interval (grey band around the blue line), and a prediction interval (red dotted line). A prediction interval reflects the uncertainty around a single value (put simple: what is the reasonable upper and lower bound that this data point could be estimated at?). A confidence interval reflects the uncertainty around the mean prediction values (put simply: what is a reasonable upper and lower bound for the blue line at this x value?). Therefore, a prediction interval will be generally much wider than a confidence interval for the same value. 2.5 Analyse the residuals #Add the residuals to the series residuals_temp_predict &lt;- MEL_weather_2019 %&gt;% add_residuals(temp_model) Plot these residuals in a chart. residuals_temp_predict_chart &lt;- ggplot(data=residuals_temp_predict, aes(x=Solar_exposure, y=resid), col=&quot;grey&quot;)+ geom_ref_line(h=0,colour=&quot;blue&quot;, size=1)+ geom_point(col=&quot;grey&quot;)+ xlab(&quot;Solar exposure&quot;)+ ylab(&quot;Maximum temperature (°C)&quot;)+ theme_bw() + labs(title = &quot;Residual values from the linear model&quot;)+ theme(axis.text=element_text(size=12))+ scale_x_continuous(expand=c(0,0)) residuals_temp_predict_chart 2.6 Linear regression with more than one variable The linear model above is *okay*, but can we make it better? Let’s start by adding in some more variables into the linear regression. Rainfall data might assist our model in predicting temperature. Let’s add in that variable and analyse the results. temp_model_2 &lt;- lm(Max_temp ~ Solar_exposure + Rainfall, data=MEL_weather_2019) summary(temp_model_2) We can see that adding in rainfall made the model better (R squared value has increased to 0.4338). Next, we consider whether solar exposure and rainfall might be related to each other, as well as to temperature. For our third temperature model, we add an interaction variable between solar exposure and rainfall. temp_model_3 &lt;- lm(Max_temp ~ Solar_exposure + Rainfall + Solar_exposure:Rainfall, data=MEL_weather_2019) summary(temp_model_3) We now see this variable is significant, and improves the model slightly (seen by an adjusted R squared of 0.4529). 2.7 Fitting a polynomial regression When analysing the above data set, we see the issue is the sheer variance of temperatures associated with every other variable (it turns out weather forecasting is notoriously difficult). However we can expect that temperature follows a non-linear pattern throughout the year (in Australia it is hot in January-March, cold in June-August, then starts to warm up again). A linear model (e.g. a straight line) will be a very bad model for temperature — we need to introduce polynomials. For simplicity, we will introduce a new variable (Day_number) which is the day of the year (e.g. 1 January is #1, 31 December is #366). MEL_weather_2019 &lt;- MEL_weather_2019 %&gt;% mutate(Day_number=row_number()) head(MEL_weather_2019) Using the same dataset as above, let’s plot temperature in Melbourne in 2019. MEL_temp_chart &lt;- ggplot(MEL_weather_2019)+ geom_line(aes(x = Day_number, y = Max_temp)) + labs(title = &#39;Melbourne temperature profile&#39;, subtitle = &#39;Daily maximum temperature recorded in Melbourne in 2019&#39;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlab(&quot;Day of the year&quot;)+ ylab(&quot;Temperature&quot;)+ theme_bw() MEL_temp_chart We can see we’ll need a non-linear model to fit this data. Below we create a few different models. We start with a normal straight line model, then add an x² and x³ model. We then use these models and the ‘predict’ function to see what temperatures they forecast based on the input data. #Create a straight line estimate to fit the data poly1 &lt;- lm(Max_temp ~ poly(Day_number,1,raw=TRUE), data=MEL_weather_2019) summary(poly1) #Create a polynominal of order 2 to fit this data poly2 &lt;- lm(Max_temp ~ poly(Day_number,2,raw=TRUE), data=MEL_weather_2019) summary(poly2) #Create a polynominal of order 3 to fit this data poly3 &lt;- lm(Max_temp ~ poly(Day_number,3,raw=TRUE), data=MEL_weather_2019) summary(poly3) #Use these models to predict MEL_weather_2019 &lt;- MEL_weather_2019 %&gt;% mutate(poly1values=predict(poly1,newdata=MEL_weather_2019))%&gt;% mutate(poly2values=predict(poly2,newdata=MEL_weather_2019))%&gt;% mutate(poly3values=predict(poly3,newdata=MEL_weather_2019)) head(MEL_weather_2019) In the table above we can see the estimates for that data point from the various models. To see how well the models did graphically, we can plot the original data series with the polynominal models overlaid. #Plot a chart with all models on it MEL_weather_model_chart &lt;- ggplot(MEL_weather_2019)+ geom_line(aes(x=Day_number, y= Max_temp),col=&quot;grey&quot;)+ geom_line(aes(x=Day_number, y= poly1values),col=&quot;red&quot;) + geom_line(aes(x=Day_number, y= poly2values),col=&quot;green&quot;)+ geom_line(aes(x=Day_number, y= poly3values),col=&quot;blue&quot;)+ #Add text annotations geom_text(x=10,y=18,label=&quot;data series&quot;,col=&quot;grey&quot;,hjust=0)+ geom_text(x=10,y=16,label=&quot;linear&quot;,col=&quot;red&quot;,hjust=0)+ geom_text(x=10,y=13,label=parse(text=&quot;x^2&quot;),col=&quot;green&quot;,hjust=0)+ geom_text(x=10,y=10,label=parse(text=&quot;x^3&quot;),col=&quot;blue&quot;,hjust=0)+ labs(title = &quot;Estimating Melbourne&#39;s temperature&quot;, subtitle = &#39;Daily maximum temperature recorded in Melbourne in 2019&#39;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlim(0,366)+ ylim(10,45)+ scale_x_continuous(breaks= c(15,45,75,105,135,165,195,225,255,285,315,345), labels=c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;), expand=c(0,0), limits=c(0,366)) + scale_y_continuous(breaks=c(10,15,20,25,30,35,40,45)) + xlab(&quot;&quot;)+ ylab(&quot;°C&quot;)+ theme_bw()+ theme(axis.text=element_text(size=12))+ theme(panel.grid.minor = element_blank()) MEL_weather_model_chart We can see in the chart above the polynomial models do much better at fitting the data. However, they are still highly variant. Just how variant are they? We can look at the residuals to find out. The residuals is the gap between the observed data point (i.e. the grey line) and our model. #Get the residuals for poly1 residuals_poly1 &lt;- MEL_weather_2019 %&gt;% add_residuals(poly1) residuals_poly1_chart &lt;- ggplot(data=residuals_poly1,aes(x=Day_number, y=resid))+ geom_ref_line(h=0,colour=&quot;red&quot;, size=1)+ geom_line()+ xlab(&quot;&quot;)+ ylab(&quot;°C&quot;)+ theme_bw()+ theme(axis.text=element_text(size=12))+ theme(axis.ticks.x=element_blank(), axis.text.x=element_blank()) residuals_poly1_chart #Get the residuals for poly2 residuals_poly2 &lt;- MEL_weather_2019%&gt;% add_residuals(poly2) residuals_poly2_chart &lt;- ggplot(data=residuals_poly2,aes(x=Day_number, y=resid))+ geom_ref_line(h=0,colour=&quot;green&quot;, size=1)+ geom_line()+ xlab(&quot;&quot;)+ ylab(&quot;°C&quot;)+ theme_bw()+ theme(axis.text=element_text(size=12))+ theme(axis.ticks.x=element_blank(), axis.text.x=element_blank()) residuals_poly2_chart #Get the residuals for poly3 residuals_poly3 &lt;- MEL_weather_2019 %&gt;% add_residuals(poly3) residuals_poly3_chart &lt;- ggplot(data=residuals_poly3,aes(x=Day_number, y=resid))+ geom_ref_line(h=0,colour=&quot;blue&quot;, size=1)+ geom_line()+ theme_bw()+ theme(axis.text=element_text(size=12))+ scale_x_continuous(breaks= c(15,45,75,105,135,165,195,225,255,285,315,345), labels=c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;), expand=c(0,0), limits=c(0,366))+ xlab(&quot;&quot;)+ ylab(&quot;°C&quot;) residuals_poly3_chart three_charts_single_page &lt;- plot_grid( residuals_poly1_chart, residuals_poly2_chart, residuals_poly3_chart, ncol=1,nrow=3,label_size=16) three_charts_single_page As we move from a linear, to a x², to a x³ model, we see the residuals decrease in volatility. 2.8 — "],["charts.html", "Chapter 3 Charts 3.1 Getting started 3.2 Make the data tidy 3.3 Line plot 3.4 Scatter and trend plot 3.5 Shading areas on plots 3.6 Bar chart (numercial) 3.7 Stacked bar chart 3.8 Histogram 3.9 Ridge chart 3.10 BBC style: Bar charts (categorical) 3.11 BBC style: Dumbbell charts 3.12 Facet wraps 3.13 Pie chart 3.14 Patchwork 3.15 Saving to powerpoint 3.16 Automating chart creation 3.17 —", " Chapter 3 Charts 3.1 Getting started There’s exceptional resources online for using the ggplot2 package to create production ready charts. The R Graph Gallery is a great place to start, as is the visual storytelling blogs of The Economist and the BBC. This chapter contains the code for some of my most used charts and visualization techniques. # Load in packages library(ggridges) library(ggplot2) library(ggrepel) library(viridis) library(readxl) library(hrbrthemes) library(dplyr) library(stringr) library(reshape) library(tidyr) library(lubridate) library(gapminder) library(grattantheme) library(ggalt) library(purrr) library(scales) library(purrr) #library(bbplot) 3.2 Make the data tidy Before making a chart ensure the data is “tidy” - meaning there is a new row for every changed variable. It also doesn’t hurt to remove NA’s for consistency (particularly in time series). #Read in data url &lt;-&quot;https://raw.githubusercontent.com/charlescoverdale/ggridges/master/2019_MEL_max_temp_daily.xlsx&quot; #Read in with read.xlsx MEL_temp_daily &lt;- openxlsx::read.xlsx(url) #Remove last 2 characters to just be left with the day number MEL_temp_daily$Day=substr(MEL_temp_daily$Day,1,nchar(MEL_temp_daily$Day)-2) #Make a wide format long using the gather function MEL_temp_daily &lt;- MEL_temp_daily %&gt;% gather(Month,Temp,Jan:Dec) MEL_temp_daily$Month&lt;-factor(MEL_temp_daily$Month,levels=c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;)) #Add in a year MEL_temp_daily[&quot;Year&quot;]=2019 #Reorder MEL_temp_daily &lt;- MEL_temp_daily[,c(1,2,4,3)] #Make a single data field using lubridate MEL_temp_daily &lt;- MEL_temp_daily %&gt;% mutate(Date = make_date(Year, Month, Day)) #Drop the original date columns MEL_temp_daily &lt;- MEL_temp_daily %&gt;% dplyr::select(Date, Temp) %&gt;% drop_na() #Add on a 7-day rolling average MEL_temp_daily &lt;- MEL_temp_daily %&gt;% dplyr::mutate(Seven_day_rolling = zoo::rollmean(Temp, k = 7, fill = NA), Mean = mean(Temp)) #Drop NA&#39;s #MEL_temp_daily &lt;- MEL_temp_daily %&gt;% drop_na() 3.3 Line plot plot_MEL_temp &lt;- ggplot(MEL_temp_daily)+ geom_line(aes(x = Date, y = Temp), col = &quot;blue&quot;)+ geom_line(aes(x = Date, y = Mean), col = &quot;orange&quot;)+ labs(title=&quot;Hot in the summer and cool in the winter&quot;, subtitle = &quot;Analysing temperature in Melbourne&quot;, caption = &quot;Data: Bureau of Meterology 2019&quot;, x=&quot;&quot;, y=&quot;&quot;) + scale_x_date(date_breaks = &quot;1 month&quot;, date_labels = &quot;%b&quot;, limits = as.Date(c(&#39;2019-01-01&#39;,&#39;2019-12-14&#39;)))+ scale_y_continuous(label = unit_format(unit=&quot;\\u00b0C&quot;, sep=&quot;&quot;)) + theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -20)))+ theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4))+ annotate(geom=&#39;curve&#39;, x=as.Date(&#39;2019-08-01&#39;), y=23, xend=as.Date(&#39;2019-08-01&#39;),yend=17, curvature=-0.5,arrow=arrow(length=unit(2,&quot;mm&quot;)))+ annotate(geom=&#39;text&#39;,x=as.Date(&#39;2019-07-15&#39;),y=25, label=&quot;Below 20°C all winter&quot;) plot_MEL_temp 3.4 Scatter and trend plot MEL_temp_Jan &lt;- MEL_temp_daily %&gt;% filter(MEL_temp_daily$Date&lt;as.Date(&quot;2019-01-31&quot;)) ggplot(MEL_temp_Jan)+ geom_point(aes(x = Date, y = Temp), col = &quot;purple&quot;,alpha=0.4)+ geom_smooth(aes(x = Date, y = Temp), col = &quot;purple&quot;,fill=&quot;purple&quot;, alpha=0.1,method = &quot;lm&quot;)+ #scale_colour_manual(values = c(&quot;purple&quot;),labels=&quot;Trend&quot;) + #scale_fill_manual(values = c(&quot;purple&quot;),labels=&quot;Confidence interval&quot;)+ labs(title=&quot;January is a hot one&quot;, subtitle = &quot;Analysing temperature in Melbourne&quot;, caption = &quot;Data: Bureau of Meterology 2019&quot;, x=&quot;&quot;, y=&quot;Temperature °C&quot;) + scale_x_date(date_breaks = &quot;1 week&quot;, date_labels = &quot;%d-%b&quot;, limits = as.Date(c(&#39;2019-01-01&#39;,&#39;2019-01-31&#39;)), expand = c(0,0)) + theme_minimal() + theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(axis.title.y = element_text(size=9,margin=margin(t = 0, r = 10, b = 0, l = 0)))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4))+ geom_hline(yintercept=45,colour =&quot;black&quot;,size=0.4) + annotate(geom=&#39;curve&#39;, x=as.Date(&#39;2019-01-22&#39;), y=37.5, xend=as.Date(&#39;2019-01-25&#39;),yend=42, curvature=0.5, col=&quot;#575757&quot;, arrow=arrow(length=unit(2,&quot;mm&quot;))) + annotate(geom=&#39;text&#39;,x=as.Date(&#39;2019-01-16&#39;),y=37.5, label=&quot;January saw some extreme temperatures&quot;,size=3.2,col=&quot;#575757&quot;) 3.5 Shading areas on plots Adding shading behind a plot area is simple using geom_rect. Adding shading under a particular model line? A little trickier. See both example below. #Example 1 #Set a custom limit for the y-axis threshold threshold &lt;- 20 ggplot() + geom_point(data = mtcars, aes(x = hp, y = mpg)) + #Add an intercept line geom_hline(yintercept = threshold) + # Shade area under y_lim geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = threshold), alpha = 1/5, fill = &quot;blue&quot;) + # Shade area above y_lim geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = threshold, ymax = Inf), alpha = 1/5, fill = &quot;red&quot;) #Example 2 #Now to add colour either way of a model line #Define the model model &lt;- lm(mpg ~ log(hp), data = mtcars) # Get the predictions for plotting. Here, df_line, is a data frame with new # coordinates that will be used for plotting the trend line and further for # building the polygons for shading. min_x &lt;- min(mtcars$hp) max_x &lt;- max(mtcars$hp) df_line &lt;- data.frame(hp = seq(from = min_x, to = max_x, by = 1)) df_line$mpg &lt;- predict(model, newdata = df_line) p &lt;- ggplot() + geom_point(data = mtcars, aes(x = hp, y = mpg),color=&quot;grey&quot;,alpha=0.5) + geom_line(data = df_line, aes(x = hp, y = mpg),col=&quot;black&quot;) #Define two polygons (one above, and one below the line) df_poly_under &lt;- df_line %&gt;% tibble::add_row(hp = c(max_x, min_x), mpg = c(-Inf, -Inf)) df_poly_above &lt;- df_line %&gt;% tibble::add_row(hp = c(max_x, min_x), mpg = c(Inf, Inf)) #Plot the data, line, and the shades above and below the line p + geom_polygon(data = df_poly_under, aes(x = hp, y = mpg), fill = &quot;blue&quot;, alpha = 1/5) + geom_polygon(data = df_poly_above, aes(x = hp, y = mpg), fill = &quot;red&quot;, alpha = 1/5)+ scale_x_discrete(expand = c(0,0))+ theme_minimal() + labs(title=&quot;Look at that snazzy red/blue shaded area&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())+ theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) 3.6 Bar chart (numercial) Year = c(&quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;) Value = (c(1000000,3000000, 2000000, 5000000)) bar_data_single &lt;- (cbind(Year, Value)) bar_data_single &lt;- as.data.frame(bar_data_single) bar_data_single$Value = as.integer(bar_data_single$Value) ggplot(bar_data_single, aes(x = Year, y = Value, label=Value)) + geom_bar(stat=&#39;identity&#39;,fill=&quot;blue&quot;,width=0.8)+ geom_text(size = 5, col=&quot;white&quot;,fontface=&quot;bold&quot;, position = position_stack(vjust = 0.5), label=scales::dollar(Value,scale=1/1e6,suffix=&quot;m&quot;))+ labs(title=&quot;Bar chart example&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=12))+ theme(axis.text=element_text(size=12))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank())+ theme(panel.grid.major.y = element_blank()) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) # ggsave(plot=last_plot(), # width=10, # height=10, # units=&quot;cm&quot;, # dpi = 600, # filename = &quot;/Users/charlescoverdale/Desktop/test.png&quot;) 3.7 Stacked bar chart Year = c(&quot;2019&quot;, &quot;2019&quot;, &quot;2019&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2020&quot;, &quot;2020&quot;, &quot;2020&quot;) Quarter = c(&quot;Q1&quot;,&quot;Q2&quot;,&quot;Q3&quot;, &quot;Q4&quot;,&quot;Q1&quot;,&quot;Q2&quot;,&quot;Q3&quot;, &quot;Q4&quot;) Value = (c(100,300,200,500,400,700,200,300)) bar_data &lt;- (cbind(Year, Quarter, Value)) bar_data &lt;- as.data.frame(bar_data) bar_data$Value = as.integer(bar_data$Value) bar_data_totals &lt;- bar_data %&gt;% dplyr::group_by(Year) %&gt;% dplyr:: summarise(Total = sum(Value)) ggplot(bar_data, aes(x = Year, y = Value, fill = (Quarter), label=Value)) + geom_bar(position = position_stack(reverse=TRUE),stat=&#39;identity&#39;)+ geom_text(size = 4, col=&quot;white&quot;, fontface=&quot;bold&quot;, position = position_stack(reverse=TRUE,vjust = 0.5), label=scales::dollar(Value))+ geom_text(aes(Year, Total, label=scales::dollar(Total), fill = NULL, vjust=-0.5), fontface=&quot;bold&quot;, size=4, data = bar_data_totals)+ scale_fill_brewer(palette = &quot;Blues&quot;) + labs(title=&quot;Bar chart example&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;Units&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;)+ theme(legend.title = element_blank())+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=10))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=10))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank())+ theme(panel.grid.major.y = element_blank()) + scale_y_continuous(expand=c(0,0),limits=c(0,1800))+ theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) 3.8 Histogram Aka. a bar chart for a continuous variable where the bars are touching. Useful to show distribution of time series or ordinal variables. c(&quot;0-20&quot;, &quot;20-40&quot;, &quot;40-60&quot;, &quot;60-80&quot;, &quot;80+&quot;) ## [1] &quot;0-20&quot; &quot;20-40&quot; &quot;40-60&quot; &quot;60-80&quot; &quot;80+&quot; #Create a data set hist_data &lt;- data.frame(X1=sample(0:100,100,rep=TRUE)) ggplot(hist_data)+ geom_histogram(aes(x=X1),binwidth=5,fill=&quot;blue&quot;,alpha=0.5) + geom_vline(xintercept=c(50,75,95),yintercept=0,linetype=&quot;longdash&quot;,col=&quot;orange&quot;) + labs(title=&quot;Histogram example&quot;, subtitle = &quot;Facet wraps are looking good&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.subtitle = element_text(margin=margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) 3.9 Ridge chart Handy when working with climate variables. Particularly useful at showing the difference in range of multiples series (e.g. temperature by month). # Import data url &lt;-&quot;https://raw.githubusercontent.com/charlescoverdale/ggridges/master/2019_MEL_max_temp_daily.xlsx&quot; MEL_temp_daily &lt;- openxlsx::read.xlsx(url) # Remove last 2 characters to just be left with the day number MEL_temp_daily$Day=substr(MEL_temp_daily$Day,1,nchar(MEL_temp_daily$Day)-2) # Make a wide format long using the gather function MEL_temp_daily &lt;- MEL_temp_daily %&gt;% gather(Month,Temp,Jan:Dec) MEL_temp_daily$Month&lt;-factor(MEL_temp_daily$Month,levels=c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;)) # Plot ggplot(MEL_temp_daily, aes(x = Temp, y = Month, fill = stat(x))) + geom_density_ridges_gradient(scale =2, size=0.3, rel_min_height = 0.01, gradient_lwd = 1.) + scale_y_discrete(limits = unique(rev(MEL_temp_daily$Month)))+ scale_fill_viridis_c(name = &quot;°C&quot;, option = &quot;C&quot;) + labs(title = &#39;Melbourne temperature profile&#39;, subtitle = &#39;Daily maximum temperature recorded in Melbourne in 2019&#39;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlab(&quot; &quot;)+ ylab(&quot; &quot;)+ theme_ridges(font_size = 13, grid = TRUE) 3.10 BBC style: Bar charts (categorical) # #Prepare data # bar_df &lt;- gapminder %&gt;% # filter(year == 2007 &amp; continent == &quot;Africa&quot;) %&gt;% # arrange(desc(lifeExp)) %&gt;% # head(5) # # #Make plot # bars &lt;- ggplot(bar_df, aes(x = country, y = lifeExp)) + # geom_bar(stat=&quot;identity&quot;, # position=&quot;identity&quot;, # fill=ifelse(bar_df$country == &quot;Mauritius&quot;, &quot;#1380A1&quot;, &quot;#dddddd&quot;)) + # geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + # bbc_style() + # labs(title=&quot;Reunion is highest&quot;, # subtitle = &quot;Highest African life expectancy, 2007&quot;) # # bars &lt;- bars + coord_flip() # bars &lt;- bars + coord_flip() + # theme(panel.grid.major.x = element_line(color=&quot;#cbcbcb&quot;), # panel.grid.major.y=element_blank()) # bars &lt;- bars + scale_y_continuous(limits=c(0,85), # breaks = seq(0, 80, by = 20), # labels = c(&quot;0&quot;,&quot;20&quot;, &quot;40&quot;, &quot;60&quot;, &quot;80 years&quot;)) # # labelled.bars &lt;- bars + # geom_label(aes(x = country, y = lifeExp, label = round(lifeExp, 0)), # hjust = 1, # vjust = 0.5, # colour = &quot;white&quot;, # fill = NA, # label.size = NA, # family=&quot;Helvetica&quot;, # size = 6) # # labelled.bars 3.11 BBC style: Dumbbell charts Dumbbell charts are handy instead of using clustered column charts with janky thinkcell labels and arrows to show the difference between the columns. Note it relies on having a 4 variable input (variable_name, value1, value2, and gap). The geom_dumbellfunction lives inside the ggalt package rather than the standard ggplot2. # #Prepare data # dumbbell_df &lt;- gapminder %&gt;% # filter(year == 1967 | year == 2007) %&gt;% # select(country, year, lifeExp) %&gt;% # spread(year, lifeExp) %&gt;% # mutate(gap = `2007` - `1967`) %&gt;% # arrange(desc(gap)) %&gt;% # head(10) # # #Make plot # ggplot(dumbbell_df, aes(x = `1967`, xend = `2007`, y = reorder(country, gap), group = country)) + geom_dumbbell(colour = &quot;#dddddd&quot;, # size = 3, # colour_x = &quot;#FAAB18&quot;, # colour_xend = &quot;#1380A1&quot;) + # bbc_style() + # labs(title=&quot;We&#39;re living longer&quot;, # subtitle=&quot;Biggest life expectancy rise, 1967-2007&quot;) 3.12 Facet wraps Handy rather than showing multiple lines on the same chart. Top tips: facet_wrap()dataframes need to be in long form in order to be manipulated easily. It also helps to add on separate columns for the start and end values (if you want to add data point labels). #Create a data set Year = c(&quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;) QLD = (c(500,300, 500, 600)) NSW = (c(200,400, 500, 700)) VIC = (c(300,400, 500, 600)) #Combine the columns into a single dataframe facet_data &lt;- (cbind(Year, QLD,NSW,VIC)) facet_data &lt;- as.data.frame(facet_data) #Change formats to integers facet_data$QLD = as.integer(facet_data$QLD) facet_data$NSW = as.integer(facet_data$NSW) facet_data$VIC = as.integer(facet_data$VIC) #Make the wide data long facet_data_long &lt;- pivot_longer(facet_data,!Year, names_to=&quot;State&quot;, values_to=&quot;Value&quot;) facet_data_long &lt;- facet_data_long %&gt;% dplyr::mutate(start_label = if_else(Year == min(Year), as.integer(Value), NA_integer_)) facet_data_long &lt;- facet_data_long %&gt;% dplyr::mutate(end_label = if_else(Year == max(Year), as.integer(Value), NA_integer_)) #Make the base line chart base_chart &lt;- ggplot() + geom_line(data=facet_data_long, aes(x = Year, y = Value, group = State, colour = State)) + geom_point(data=facet_data_long, aes(x = Year, y = Value, group = State, colour = State)) + ggrepel::geom_text_repel(data=facet_data_long, aes(x = Year, y = Value, label = end_label), color = &quot;black&quot;, nudge_y = -10,size=3) + ggrepel::geom_text_repel(data=facet_data_long, aes(x = Year, y = Value, label = start_label), color = &quot;black&quot;, nudge_y = 10,size=3) base_chart + scale_x_discrete( breaks = seq(2018, 2021, 1), labels = c(&quot;2018&quot;, &quot;19&quot;, &quot;20&quot;, &quot;21&quot;))+ facet_wrap(State ~ .) + #To control the grid arrangement, we can add in customer dimensions #ncol = 2, nrow=2) + labs(title=&quot;State by state comparison&quot;, subtitle = &quot;Facet wraps are looking good&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(strip.text.x = element_text(size = 9, face = &quot;bold&quot;)) + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.subtitle = element_text(margin=margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) 3.13 Pie chart These should be used sparingly… but they are handy for showing proportions when the proportion of the whole is paramount (e.g. 45%) - rather than the proportion in relation to another data point (e.g. 16% one year vs 18% the next). # Create Data pie_data &lt;- data.frame( group=LETTERS[1:5], value=c(13,7,9,21,2)) # Compute the position of labels pie_data &lt;- pie_data %&gt;% arrange(desc(group)) %&gt;% mutate(proportion = value / sum(pie_data$value) *100) %&gt;% mutate(ypos = cumsum(proportion)- 0.5*proportion ) # Basic piechart ggplot(pie_data, aes(x=&quot;&quot;, y=proportion, fill=group)) + geom_bar(stat=&quot;identity&quot;)+ coord_polar(&quot;y&quot;, start=0) + theme_void()+ geom_text(aes(y = ypos, label = paste(round(proportion,digits=0),&quot;%&quot;, sep = &quot;&quot;),x=1.25), color = &quot;white&quot;, size=4) + scale_fill_brewer(palette=&quot;Set1&quot;)+ labs(title=&quot;Use pie charts sparingly&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme(legend.position = &quot;bottom&quot;)+ theme(legend.title = element_blank())+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(plot.subtitle = element_text(margin=margin(0,0,5,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) # ggsave(plot=last_plot(), # width=10, # height=10, # units=&quot;cm&quot;, # dpi = 600, # filename = &quot;/Users/charlescoverdale/Desktop/pietest.png&quot;) 3.14 Patchwork Patchwork is a nifty package for arranging plots and other graphic elements (text, tables etc) in different grid arrangements. The basic syntax is to use plot1 | plot2 for side by side charts, and plot1 / plot2for top and bottom charts. You can also combine these two functions for a grid of different size columns (e.g. plot3 / (plot1 | plot2) #Make some simply plots using the mtcars package p1 &lt;- ggplot(mtcars) + geom_point(aes(mpg, disp)) + ggtitle(&#39;Plot 1&#39;) p2 &lt;- ggplot(mtcars) + geom_boxplot(aes(gear, disp, group = gear)) + ggtitle(&#39;Plot 2&#39;) #Example of side by side charts p1 + p2 #Add in a table next to the plot p1 + gridExtra::tableGrob(mtcars[1:10, c(&#39;mpg&#39;, &#39;disp&#39;)]) 3.15 Saving to powerpoint There’s a bunch of ways to save ggplot graphics - but the way I find most useful is by exporting to pptx in a common ‘charts’ directory. If you want to save as a png you can use the normal ggsave function - however it will not be editable (e.g. able to click and drag to rescale for a presentation). Therefore instead we can use the grattantheme package to easily save to an editable pptx graphic. Note: The code below has been commented out so that is will upload to bookdown.org without an error. #The classic save function to png # ggsave(plot = ggplot2::last_plot(), # width = 8, # height = 12, # dpi = 600, # filename = &quot;/Users/charlescoverdale/Desktop/test.png&quot;) #Using the grattantheme package to easily safe to powerpoint # grattan_save_pptx(p = ggplot2::last_plot(), # &quot;/Users/charlescoverdale/Desktop/test.pptx&quot;, # type = &quot;wholecolumn&quot;) 3.16 Automating chart creation Let’s say we have a dataframe of multiple variables. We want to produce simple charts of the same style for each variable (including formatting and titles etc). Sure we can change the aes(x=)and aes(y=) variables in ggplot2 manually for each column - but this is time intensive especially for large data frames. Instead, we can write a function that will loop through the whole dataframe and produce the same format of chart. #Create a data set Year = c(&quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;) Variable1 = (c(500,300, 200, 400)) Variable2 = (c(200,400, 200, 700)) Variable3 = (c(300,500, 800, 1000)) #Combine the columns into a single dataframe bar_data_multiple &lt;- (cbind(Year, Variable1, Variable2, Variable3)) bar_data_multiple &lt;- as.data.frame(bar_data_multiple) #Change formats to integers bar_data_multiple$Variable1 = as.integer(bar_data_multiple$Variable1) bar_data_multiple$Variable2 = as.integer(bar_data_multiple$Variable2) bar_data_multiple$Variable3 = as.integer(bar_data_multiple$Variable3) #Define a function loop &lt;- function(chart_variable) { ggplot(bar_data_multiple, aes(x = Year, y = .data[[chart_variable]], label = .data[[chart_variable]]))+ geom_bar(stat=&#39;identity&#39;,fill=&quot;blue&quot;)+ geom_text(aes( label = scales::dollar(.data[[chart_variable]])), size = 5, col=&quot;white&quot;, fontface=&quot;bold&quot;, position = position_stack(vjust = 0.5))+ labs(title=paste(&quot;Company X: &quot;, chart_variable, &quot; (&quot;,head(Year,n=1), &quot; - &quot;, tail(Year,n=1), &quot;)&quot;, sep=&quot;&quot;), subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=12))+ theme(axis.text=element_text(size=12))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank())+ theme(panel.grid.major.y = element_blank()) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) } plots &lt;- purrr::map(colnames(bar_data_multiple)[colnames(bar_data_multiple) != &quot;Year&quot;], loop) plots #cowplot::plot_grid(plotlist = plots) 3.17 — "],["hypothesis-testing.html", "Chapter 4 Hypothesis testing 4.1 A quick refresher 4.2 T-testing our first hypothesis 4.3 Tailed tests 4.4 Correlation (and working with normal distributions) 4.5 Confidence intervals (mean) 4.6 Confidence intervals (model)", " Chapter 4 Hypothesis testing 4.1 A quick refresher Hypothesis testing is a way of validating if a claim about a population (e.g. a data set) is correct. Getting data on a whole population (e.g. everyone in Australia) is hard - so to validate a hypothesis, we use random samples from a population instead. The language when dealing with hypothesis testing is purposefully janky. When looking at the outputs of our hypothesis test, we consider p-values. Note: There’s lots wrong with p-values that we won’t bother getting into right now. The long story short is if you make your null hypothesis ultra specific and only report when your p-value on your millionth iteration of a test is below 0.05… bad science is likely to get published and cited. What we need to know: A small p-value (typically ≤ 0.05) indicates strong evidence against the null hypothesis, so we reject it. A large p-value (&gt; 0.05) indicates weak evidence against the null hypothesis, so you fail to reject it. Let’s load in some packages and get started. # Load in packages library(ggridges) library(ggplot2) library(forecast) library(ggrepel) library(viridis) library(readxl) library(hrbrthemes) library(dplyr) library(stringr) library(reshape) library(tidyr) library(lubridate) library(gapminder) library(grattantheme) library(ggalt) library(purrr) library(scales) library(purrr) library(aTSA) library(readrba) 4.2 T-testing our first hypothesis We’ll start simple. Let’s create a random dataset - with the caveat that it will be normally distributed. By default the rnorm function will generate a dataset that has a mean of 0 and a standard deviation of 1. set.seed(40) dataset1 &lt;- data.frame(variable1=rnorm(1000,mean=0,sd=1)) ggplot()+ geom_histogram(aes(x=dataset1$variable1,y=..density..),binwidth=0.1,fill=&quot;blue&quot;,alpha=0.5) + stat_function(fun = dnorm, args = list(mean = mean(dataset1$variable1), sd = sd(dataset1$variable1)))+ geom_hline(yintercept = 0) + geom_vline(xintercept = 0, linetype=&quot;dotted&quot;,alpha=0.5)+ labs(title=&quot;Histogram for t-testing&quot;, caption = &quot;Data: Made from rnorm(1000)&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + scale_x_continuous(breaks = seq(-3, 3, by = 1))+ theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.subtitle = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) We know by default that the mean of dataset1 will be approximately zero… but let’s check anyway. # Find the mean of dataset1 mean(dataset1$variable1) Great, now let’s run our first hypothesis test. We’ll use the t.test function. This is in the format of t.test(data, null_hypothesis). So quite simply, we can test the null hypothesis that the mean for dataset1$variable1 is 5). # Hypothesis test t.test(dataset1$variable1, mu = 5) We see the p-value here is tiny, meaning we reject the null hypothesis. That is to say, the mean for dataset1$variable1 is not 5. Going further, let’s test the hypothesis that the mean is 0.01. # Hypothesis test t.test(dataset1$variable1, mu = -0.03, alternative=&quot;greater&quot;) We see here the p-value is greater than 0.05, leading us to fail to reject the null hypothesis. In a sentence, we cannot say that the mean of dataset1$variable1 is different to 0.01. 4.3 Tailed tests In the previous level we were cheating slightly, in that we didn’t specify the ‘tail’ for the test, or our confidence level. By default the t.test function assumes tests are two tailed, and the desired confidence level is 0.95 (i.e. 95%). However, in some cases we might want to have a hypothesis that says one variable is greater than or less than another variable (rather than just different from each other). This is where we use tails. # Hypothesis test t.test(dataset1$variable1, mu = 0.03, alternative = &quot;greater&quot;) 4.4 Correlation (and working with normal distributions) A correlation coefficient measures the direction and strength of the correlation between two variables. The tricky thing is - if variables aren’t normally distributed, none of our correlation theory works very well. In the example below, we see that miles per gallon is correlated with horsepower. It’s a negative relationship, meaning the more horsepower in a car, the less miles per gallon the car exhibits. ggplot(mtcars)+ geom_point(aes(x = hp, y = mpg), col = &quot;blue&quot;, alpha=0.5)+ geom_smooth(aes(x = hp, y = mpg),method=&#39;lm&#39;)+ labs(title=&quot;Building a regression model&quot;, subtitle = &quot;Higher horsepower cars get less miles to the gallon&quot;, caption = &quot;Data: mtcars dataset&quot;, x=&quot;Horsepower&quot;, y=&quot;Miles per gallon&quot;) + theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 3, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)))+ theme(plot.subtitle = element_text(margin=ggplot2::margin(0,0,25,0))) + theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 3, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)))+ theme(plot.subtitle = element_text(margin=ggplot2::margin(0,0,25,0))) + theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4)) We can use the cor.test to tell us the correlation coefficient and the p-value of the correlation. We specify the method as ‘pearson’ for the Pearson correlation coefficient. But we remember this method only works well if both our variables are normally distributed. That’s worth checking - let’s plot a histogram for both hp and mpg. cor.test(mtcars$hp, mtcars$mpg, method=&quot;pearson&quot;) ggplot()+ geom_histogram(aes(x=mtcars$mpg,y=..density..),fill=&quot;blue&quot;,alpha=0.5) + stat_function(fun = dnorm, args = list(mean = mean(mtcars$mpg), sd = sd(mtcars$mpg)))+ geom_hline(yintercept = 0) + geom_vline(xintercept = mean(mtcars$mpg), linetype=&quot;dotted&quot;,alpha=0.5)+ labs(title=&quot;Histogram of mtcars$mpg&quot;, caption = &quot;Data: Made from mtcars&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) ggplot()+ geom_histogram(aes(x=mtcars$hp,y=..density..),fill=&quot;blue&quot;,alpha=0.5) + stat_function(fun = dnorm, args = list(mean = mean(mtcars$hp), sd = sd(mtcars$hp)))+ geom_hline(yintercept = 0) + geom_vline(xintercept = mean(mtcars$hp), linetype=&quot;dotted&quot;,alpha=0.5)+ labs(title=&quot;Histogram of mtcars$hp&quot;, caption = &quot;Data: Made from mtcars&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) Crikey… they don’t look very normal at all. Let’s plot QQ plots of our variables and see what’s going on. ggplot(mtcars, aes(sample = mpg)) + geom_qq()+ geom_qq_line()+ labs(title=&quot;QQ plot of mtcars$mpg&quot;, caption = &quot;Data: Made from mtcars&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) ggplot(mtcars, aes(sample = hp)) + geom_qq()+ geom_qq_line()+ labs(title=&quot;QQ plot of mtcars$hp&quot;, caption = &quot;Data: Made from mtcars&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) Hmm okay, both series are a bit all over the shop. Let’s do a statistical test to confirm. The Shapiro-Wilk’s method is widely used for normality testing. The null hypothesis of this tests is that the sample distribution is normal. If the test is significant, the distribution is non-normal. shapiro.test(mtcars$mpg) shapiro.test(mtcars$hp) 4.5 Confidence intervals (mean) Firstly, we can calculate the confidence interval on a single variable. Essentially this measures the variance of the normal distribution, and gives us an idea of how ‘clustered’ the values are to the mean. There are roughly 4 steps to do this: Calculate the mean Calculate the standard error of the mean Find the t-score that corresponds to the confidence level Calculate the margin of error and construct the confidence interval mpg.mean &lt;- mean(mtcars$mpg) print(mpg.mean) mpg.n &lt;- length(mtcars$mpg) mpg.sd &lt;- sd(mtcars$mpg) mpg.se &lt;- mpg.sd/sqrt(mpg.n) print(mpg.se) alpha = 0.05 degrees.freedom = mpg.n - 1 t.score = qt(p=alpha/2, df=degrees.freedom,lower.tail=F) print(t.score) mpg.error &lt;- t.score * mpg.se lower.bound &lt;- mpg.mean - mpg.error upper.bound &lt;- mpg.mean + mpg.error print(c(lower.bound,upper.bound)) For the lazy folks among us - there’s also this quick and dirty way of doing it. # Calculate the mean and standard error mpg.model &lt;- lm(mpg ~ 1, mtcars) # Calculate the confidence interval confint(mpg.model, level=0.95) Great. Let’s plot this interval on the distribution. ggplot()+ geom_histogram(aes(x=mtcars$mpg,y=..density..),binwidth=2,fill=&quot;blue&quot;,alpha=0.5) + stat_function(fun = dnorm, args = list(mean = mean(mtcars$mpg), sd = sd(mtcars$mpg)))+ geom_hline(yintercept = 0) + geom_vline(xintercept = mean(mtcars$mpg), linetype=&quot;dotted&quot;,alpha=0.5)+ geom_vline(xintercept = lower.bound,col=&quot;purple&quot;)+ geom_vline(xintercept = upper.bound, col=&quot;purple&quot;)+ labs(title=&quot;Histogram of mtcars$mpg&quot;, caption = &quot;Data: Made from mtcars&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) Two things we note here: First, the distribution doesn’t look that normal. Second, geez that 95% confidence interval looks narrow as a result. Let’s do the same analysis with an actual normal distribution and see what happens. set.seed(404) dataset2 &lt;- data.frame(variable1=rnorm(1000,mean=0,sd=1)) df2.mean &lt;- mean(dataset2$variable1) print(df2.mean) df2.n &lt;- length(dataset2$variable1) df2.sd &lt;- sd(dataset2$variable1) df2.se &lt;- mpg.sd/sqrt(mpg.n) print(df2.se) alpha = 0.05 degrees.freedom = df2.n - 1 t.score = qt(p=alpha/2, df=degrees.freedom,lower.tail=F) print(t.score) df2.error &lt;- t.score * df2.se lower.bound.df2 &lt;- df2.mean - df2.error upper.bound.df2 &lt;- df2.mean + df2.error print(c(lower.bound.df2,upper.bound.df2)) #Make a function to colour the tails upper_tail &lt;- function(x) { y &lt;- dnorm(x,mean=0,sd=1) y[x &lt; upper.bound.df2 | x &gt; 1000] &lt;- NA return(y) } lower_tail &lt;- function(x) { y &lt;- dnorm(x,mean=0,sd=1) y[x &lt; -1000 | x &gt; lower.bound.df2] &lt;- NA return(y) } #Plot the distributions ggplot()+ geom_histogram(aes(x=dataset2$variable1,y=..density..),binwidth=0.1,fill=&quot;blue&quot;,alpha=0.5) + stat_function(fun = dnorm, args = list(mean = mean(dataset2$variable1), sd = sd(dataset2$variable1)))+ geom_hline(yintercept = 0) + geom_vline(xintercept = mean(dataset2$variable1), linetype=&quot;dotted&quot;,alpha=0.5)+ geom_vline(xintercept = lower.bound.df2,col=&quot;purple&quot;)+ geom_vline(xintercept = upper.bound.df2, col=&quot;purple&quot;)+ stat_function(fun = upper_tail, geom = &quot;area&quot;, fill = &quot;grey&quot;,col=&quot;grey&quot;, alpha = 0.8) + stat_function(fun = lower_tail, geom = &quot;area&quot;, fill = &quot;grey&quot;,col=&quot;grey&quot;, alpha = 0.8) + labs(title=&quot;95% confidence interval&quot;, caption = &quot;Data: Made from rnorm(1000)&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + scale_x_continuous(breaks = seq(-3, 3, by = 1))+ theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) Great - we’ve got a more sensible looking plot, and greyed out the tails where our confidence interval excludes. We expect out observation to fall somewhere between the two purple lines (or more exactly between -2.1 and 2.1) 4.6 Confidence intervals (model) Secondly, we can calculate the confidence internal around a linear model. Somewhat similar to above, this process shows us how confident we can be about any single point in the linear estimate (e.g. if it has an enormous confidence interval attached to it… the linear estimate of the value at that point is probably a bit dodgy). mtcars.lm &lt;- lm(mpg ~ hp, data = mtcars) summary(mtcars.lm) predict(mtcars.lm, newdata = mtcars, interval = &#39;confidence&#39;) It’s great to have the raw data - but there’s an even easier way of plotting the confidence interval on a chart: we use the geom_smooth()function. The syntax in this example is: geom_smooth(aes(x = hp, y = mpg), method='lm', level=0.95) ggplot(mtcars)+ geom_point(aes(x = hp, y = mpg), col = &quot;blue&quot;, alpha=0.5)+ geom_smooth(aes(x = hp, y = mpg),method=&#39;lm&#39;,level=0.95)+ labs(title=&quot;Building a regression model&quot;, subtitle = &quot;Higher horsepower cars get less miles to the gallon&quot;, caption = &quot;Data: mtcars dataset&quot;, x=&quot;Horsepower&quot;, y=&quot;Miles per gallon&quot;) + theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 3, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)))+ theme(plot.subtitle = element_text(margin=ggplot2::margin(0,0,25,0))) + theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 3, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)))+ theme(plot.subtitle = element_text(margin=ggplot2::margin(0,0,25,0))) + theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4)) For a sanity check, let’s crank up the confidence level to 0.999 (meaning our interval should capture just about all the observations). We see the confidence interval band increases… but not by that much. Why? Well remember how the data isn’t a very good normal distribution? That means the confidence interval function won’t be super accurate - especially at the extremes. "],["forecasting.html", "Chapter 5 Forecasting 5.1 Background 5.2 ARIMA models", " Chapter 5 Forecasting 5.1 Background So we’ve got a time series dataset… but what is a reasonable forecast for how it might behave in the future? Sure we can do a confidence interval (as we learned in the previous chapter) - but what about forecasting for multiple periods into the future. That’s where we need to build some models. # Load in packages library(ggridges) library(ggplot2) library(forecast) library(ggrepel) library(viridis) library(readxl) library(hrbrthemes) library(dplyr) library(stringr) library(reshape) library(tidyr) library(lubridate) library(gapminder) library(grattantheme) library(ggalt) library(purrr) library(scales) library(purrr) library(aTSA) library(readrba) We’ll start with some pre-loaded time series data. The ggplot2 includes a data set called ‘economics’ that contains US economic indicators from the 1960’s to 2015. econ_data &lt;- economics %&gt;% dplyr::select(c(&quot;date&quot;, &quot;uempmed&quot;)) econ_data &lt;- econ_data %&gt;% dplyr::filter((date &gt;= as.Date(&quot;1970-01-01&quot;) &amp; date &lt;= as.Date(&quot;1999-12-31&quot;))) As a side note: We can also get Australian unemployment rate data using the readrba function. aus_unemp_rate &lt;- read_rba(series_id = &quot;GLFSURSA&quot;) head(aus_unemp_rate) Let’s plot the data to see what we are working with. ggplot(econ_data)+ geom_point(aes(x = date, y = uempmed), col = &quot;grey&quot;, alpha=0.5)+ geom_smooth(aes(x = date, y = uempmed), col = &quot;blue&quot;)+ labs(title=&quot;Unemployment rate&quot;, caption = &quot;Data: ggplot2::economics&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 3, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)))+ theme(plot.title = element_text(margin=ggplot2::margin(0,0,25,0))) + theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4)) 5.2 ARIMA models AutoRegressive Integrated Moving Average (ARIMA) models are a handy tool to have in the toolbox. An auto regressive model is one where Yt depends on its own lags. A moving average (MA only) model is one where Yt depends only on the lagged forecast errors. We combine these together (technically we integrate them) and get ARIMA. First order of business, we may need to ‘difference’ our series to make it stationary. Let’s check if it is stationary using the augmented Dickey-Fuller test. The null hypothesis assumes that the series is non-stationary. A series is said to be stationary when its mean, variance, and autocovariance don’t change much over time. # Test for stationarity aTSA::adf.test(econ_data$uempmed) ## Augmented Dickey-Fuller Test ## alternative: stationary ## ## Type 1: no drift no trend ## lag ADF p.value ## [1,] 0 -0.448 0.515 ## [2,] 1 -0.279 0.564 ## [3,] 2 -0.252 0.571 ## [4,] 3 -0.218 0.581 ## [5,] 4 -0.321 0.552 ## [6,] 5 -0.399 0.529 ## Type 2: with drift no trend ## lag ADF p.value ## [1,] 0 -3.05 0.0337 ## [2,] 1 -2.54 0.1158 ## [3,] 2 -2.43 0.1572 ## [4,] 3 -2.58 0.0986 ## [5,] 4 -2.68 0.0826 ## [6,] 5 -2.82 0.0597 ## Type 3: with drift and trend ## lag ADF p.value ## [1,] 0 -2.94 0.179 ## [2,] 1 -2.33 0.438 ## [3,] 2 -2.20 0.491 ## [4,] 3 -2.33 0.439 ## [5,] 4 -2.49 0.368 ## [6,] 5 -2.69 0.285 ## ---- ## Note: in fact, p.value = 0.01 means p.value &lt;= 0.01 # See the auto correlation acf(econ_data$uempmed) # Identify patial auto correlation Pacf(econ_data$uempmed) # Take the first differences of the series econ_data &lt;- econ_data %&gt;% mutate(diff = uempmed-lag(uempmed)) ggplot(econ_data)+ geom_point(aes(x = date, y = diff), col = &quot;grey&quot;, alpha=0.5)+ geom_smooth(aes(x = date, y = diff), col = &quot;blue&quot;)+ labs(title=&quot;1st difference (Unemployment rate)&quot;, caption = &quot;Data: ggplot2::economics&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 3, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)))+ theme(plot.title = element_text(margin=ggplot2::margin(0,0,25,0))) + theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). ARIMA_model = forecast::auto.arima(econ_data$uempmed) ARIMA_model summary(ARIMA_model) checkresiduals(ARIMA_model) # Forecast for the next 10 time units ARIMA_forecast &lt;- forecast::forecast(ARIMA_model, newdata=econ_data$uempmed,h = 36,level=c(95)) # Plot forecasts plot((ARIMA_forecast)) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

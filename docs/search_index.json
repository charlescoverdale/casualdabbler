[["index.html", "R cookbook for the casual dabbler Chapter 1 Introduction 1.1 Usage 1.2 Additional resources 1.3 Limitations 1.4 About the author", " R cookbook for the casual dabbler Charles Coverdale 2022-06-15 Chapter 1 Introduction Gday and welcome to R cookbook for the casual dabbler. Some history: I use R a lot for work and for side projects. Over the years Ive collated a bunch of useful scripts, from macroeconomic analysis to quick hacks for making map legends format properly. Historically my code has been stored in random Rpubs documents, medium articles, and a bunch of .Rmd files on my hardrive. Occasionally I feel like doing things properly - and upload code to a repository on github. It doesnt take a genius to realize this isnt a very sustainable solution - and it also isnt very useful for sharing code with others. It turns out 2-years of lockdown in Melbourne was enough incentive to sit down and collate my best and most useful code into a single place. In the spirit of open source, a book seemed like the most logical format. The following is a very rough book written in markdown - Rs very own publishing language. 1.1 Usage In each chapter Ive written up the background, methodology and code for a separate piece of analysis. Most of this code will not be extraordinary to the seasoned R aficionado. The vast majority can be found elsewhere if you dig around on stackexchange or read some of Hadleys books. However I find that in classic Pareto style ~20% of my code contributes to the vast majority of my work output. Having this on hand will hopefully be useful to both myself and others. 1.2 Additional resources The R community is continually writing new books and package documentation with great worked examples. Some of my favourites (which all happen to be written in the R markdown language) are: Geocomputation with R R Markdown: The Definite Guide R Cookbook, 2nd Edition R for Data Science Data Science in Education using R Introduction to R: Walter and Eliza Hall Institute PhD lectures notes in environmental economics and data science (University of Oregon) 1.3 Limitations Ill be honest with you - theres bound to be bugs galore in this. If you find one (along with spelling errors etc) please email me at charlesfcoverdale@gmail.com with the subject line R cookbook for the casual dabbler. 1.4 About the author Charles Coverdale is an economist based in Melbourne, Australia. He is passionate about economics, climate science, and building talented teams. You can get in touch with Charles on twitter to hear more about his current projects. "],["making-beautiful-maps-in-r.html", "Chapter 2 Making beautiful maps in R 2.1 Why use a map 2.2 Getting started 2.3 Making your first map 2.4 From okay to good 2.5 From good to great 2.6 From great to fantastic", " Chapter 2 Making beautiful maps in R 2.1 Why use a map Maps are a great way to communicate data. Theyre easily understandable, flexible, and more intuitive than a chart. Theres been numerous studies showing that the average professional often struggles to interpret the units on a y-axis, let alone understand trends in scatter or line graphs. Making maps in R takes some initial investment (note: they can be fiddly). However once you have some code you know and understand, spinning up new pieces of analysis can happen in minutes, rather than hours or days. The aim of this quick-reference guide is to get you from I can produce a map in R to something more like I can conduct spatial analysis and produce a visual which is ready to send without any further work. 2.2 Getting started First up, we need to load a bunch of packages. #Loads the required required packages library(plyr) library(dplyr) library(vctrs) library(tidyr) library(ggplot2) library(tmap) library(ggmap) library(dplyr) library(sf) library(ggspatial) library(rlang) library(broom) library(tidyverse) library(readxl) library(purrr) library(Census2016) library(absmapsdata) library(officer) library(bookdown) The absmapsdata package is particularly important, as it contains most ASGS shapefiles (the ones found on the ABS website). The main ASGS structures and their names in this package are: SA1 2016: sa12016 SA2 2016: sa22016 SA3 2016: sa32016 SA4 2016: sa42016 Greater Capital Cities 2016: gcc2016 Remoteness Areas 2016: ra2016 State 2016: state2016 2.3 Making your first map To get a basic demographic map up and running, we will splice together the ABS SA2 shapefile and some data from the 2016 Australian Census. There is a fantastic packaged called Census2016 which makes downloading this data in a clean format easy. #Get the shapefile form the absmapsdata package (predefined in the list above) #Get the 2016 census dataset census2016_wide &lt;- Census2016_wide_by_SA2_year #Select the key demographic columns from the census data (i.e. the first 8 variables) census_short &lt;- census2016_wide[,1:8] #Filter for a single year census_short_2016 &lt;- census_short %&gt;% filter(year==2016) #Use the inner_join function to get the shapefile and census wide data into a single df for analysis / visualisation SA2_shp_census_2016 &lt;- inner_join(sa22016,census_short_2016, by = c(&quot;sa2_name_2016&quot; = &quot;sa2_name&quot;)) #Plot a map that uses census data map1 &lt;- ggplot() + geom_sf(data = SA2_shp_census_2016, aes(fill = median_age)) + ggtitle(&quot;Australian median age (SA2)&quot;) + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + theme_bw() + theme(legend.position = &quot;right&quot;) map1 There we go! This looks okay but it can be much better. 2.4 From okay to good Heat maps dont really show too much interesting data on such a large scale, so lets filter down to Greater Melbourne. Seeing we have a bunch of census data in our dataframe, we can also do some basic analysis (e.g. population density). #As a bit of an added extra, we can create a new population density column SA2_shp_census_2016 &lt;- SA2_shp_census_2016 %&gt;% mutate(pop_density=persons/areasqkm_2016) #Filter for Greater Melbourne MEL_SA2_shp_census_2016 &lt;- SA2_shp_census_2016 %&gt;% filter(gcc_name_2016==&quot;Greater Melbourne&quot;) #Plot the new map just for Greater Melbourne map2 &lt;- ggplot() + geom_sf(data = MEL_SA2_shp_census_2016, aes(fill = median_age, border=NA)) + ggtitle(&quot;Median age in Melbourne (SA2)&quot;) + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + theme_bw() + theme(legend.position = &quot;right&quot;) map2 Much better. We can start to see some trends in this map. It looks like younger people tend to live closer to the city center. This seems logical. 2.5 From good to great The map above is a good start! However, how do we turn this from something good, into something that is 100% ready to share? We see our ink to chart ratio (i.e. the amount of non-data stuff that is on the page) is still pretty high. Is the latitude of Melbourne useful for this analysis? Not really. Lets get rid of it and the axis labels. A few lines of code adjusting the axis, titles, and theme of the plot will go a long way. Because my geography Professor drilled it into me, I will also add a low-key scale bar. map3 &lt;- ggplot() + geom_sf(data = MEL_SA2_shp_census_2016, aes(fill = median_age)) + labs(title=&quot;Melbourne&#39;s youth tend to live closer to the city centre&quot;, subtitle = &quot;Analysis from the 2016 census&quot;, caption = &quot;Data: Australian Bureau of Statistics 2016&quot;, x=&quot;&quot;, y=&quot;&quot;, fill=&quot;Median age&quot;) + ggspatial::annotation_scale(location=&quot;br&quot;)+ theme_minimal() + theme(axis.ticks.x = element_blank(),axis.text.x = element_blank())+ theme(axis.ticks.y = element_blank(),axis.text.y = element_blank())+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ theme(legend.position = &quot;right&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8)) map3 2.6 From great to fantastic The above is perfectly reasonable and looks professionally designed. However, this is where we can get really special. Lets add a custom colour scheme, drop the boundary edges for the SA2s, and add in a dot and label for Melbourne CBD. #Add in a point for the Melbourne CBD MEL_location &lt;- data.frame(town_name = c(&quot;Melbourne&quot;), x = c(144.9631), y = c(-37.8136)) map4 &lt;- ggplot() + geom_sf(data = MEL_SA2_shp_census_2016, aes(fill = median_age),color=NA) + geom_point(data=MEL_location,aes(x=x,y=y),size=2,color=&quot;black&quot;)+ labs(title=&quot;Melbourne&#39;s youth tend to live closer to the city centre&quot;, subtitle = &quot;Analysis from the 2016 census&quot;, caption = &quot;Data: Australian Bureau of Statistics 2016&quot;, x=&quot;&quot;, y=&quot;&quot;, fill=&quot;Median age&quot;) + scale_fill_steps(low=&quot;#E2E0EB&quot;, high=&quot;#3C33FE&quot;)+ annotate(geom=&#39;curve&#39;, x=144.9631, y=-37.8136, xend=144.9, yend=-38.05, curvature=0.5, arrow=arrow(length=unit(2,&quot;mm&quot;)))+ annotate(geom=&#39;text&#39;,x=144.76,y=-38.1,label=&quot;Melbourne CBD&quot;)+ ggspatial::annotation_scale(location=&quot;br&quot;)+ theme_minimal() + theme(axis.ticks.x = element_blank(),axis.text.x = element_blank())+ theme(axis.ticks.y = element_blank(),axis.text.y = element_blank())+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ theme(legend.position = &quot;right&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8)) map4 There we go! A client-ready looking map that can be added to a report, presentation, or with a few tweaks a digital dashboard. Make sure to export the map as a high quality PNG using the ggsave function. "],["basic-modelling-in-r.html", "Chapter 3 Basic modelling in R 3.1 Source, format, and plot data 3.2 Build a linear model 3.3 Analyse the model fit 3.4 Compare the predicted values with the actual values 3.5 Analyse the residuals 3.6 Linear regression with more than one variable 3.7 Fitting a polynomial regression", " Chapter 3 Basic modelling in R Creating a model is an essential part of forecasting and data analysis. Ive put together a quick guide on my process for modelling data and checking model fit. The source data I use in this example is Melbournes weather record over a 12 month period. Daily temperature is based on macroscale weather and climate systems, however many observable measurements are correlated (i.e. hot days tend to have lots of sunshine). This makes using weather data great for model building. 3.1 Source, format, and plot data Before we get started, it is useful to have some packages up and running. #Useful packages for regression library(readr) library(readxl) library(ggplot2) library(dplyr) library(tidyverse) library(lubridate) library(modelr) library(cowplot) Ive put together a csv file of weather observations in Melbourne in 2019. We begin our model by downloading the data from Github. #Input data link &lt;- &quot;data/MEL_weather_2019.csv&quot; # We&#39;ll read this data in as a dataframe # The &#39;check.names&#39; function set to false means the funny units that the BOM use for column names won&#39;t affect the import. MEL_weather_2019 &lt;- read.csv(link, check.names = F) head(MEL_weather_2019) This data is relatively clean. One handy change to make is to make the date into a dynamic format (to easily switch between months, years, etc). #Add a proper date column MEL_weather_2019 &lt;- MEL_weather_2019 %&gt;% mutate(Date = make_date(Year, Month, Day)) We also notice that some of the column names have symbols in them. This can be tricky to work with, so lets rename some columns into something more manageable. #Rename key df variables names(MEL_weather_2019)[4]&lt;- &quot;Solar_exposure&quot; names(MEL_weather_2019)[5]&lt;- &quot;Rainfall&quot; names(MEL_weather_2019)[6]&lt;- &quot;Max_temp&quot; head(MEL_weather_2019) Were aiming to investigate if other weather variables can predict maximum temperatures. Solar exposure seems like a plausible place to start. We start by plotting the two variables to if there is a trend. #Plot the data MEL_temp_investigate &lt;- ggplot(MEL_weather_2019)+ geom_point(aes(y=Max_temp, x=Solar_exposure),col=&quot;grey&quot;)+ labs(title = &quot;Does solar exposure drive temperature in Melbourne?&quot;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlab(&quot;Solar exposure&quot;)+ ylab(&quot;Maximum temperature °C&quot;)+ scale_x_continuous(expand=c(0,0))+ theme_bw()+ theme(axis.text=element_text(size=10))+ theme(panel.grid.minor = element_blank()) MEL_temp_investigate Eyeballing the chart above, there seems to be a correlation between the two data sets. Well do one more quick plot to analyse the data. What is the distribution of temperature? ggplot(MEL_weather_2019, aes(x=Max_temp)) + geom_histogram(aes(y=..density..), colour=&quot;black&quot;, fill=&quot;lightblue&quot;)+ geom_density(alpha=.5, fill=&quot;grey&quot;,colour=&quot;darkblue&quot;)+ scale_x_continuous(breaks=c(5,10,15,20,25,30,35,40,45), expand=c(0,0))+ xlab(&quot;Temperature&quot;)+ ylab(&quot;Density&quot;)+ theme_bw()+ theme(axis.text=element_text(size=12))+ theme(panel.grid.minor = element_blank()) We can see here the data is right skewed (i.e. the mean will be greater than the median). Well need to keep this in mind. Lets start building a model. 3.2 Build a linear model We start by looking whether a simple linear regression of solar exposure seems to be correlated with temperature. In R, we can use the linear model (lm) function. #Create a straight line estimate to fit the data temp_model &lt;- lm(Max_temp~Solar_exposure, data=MEL_weather_2019) 3.3 Analyse the model fit Lets see how well solar exposure explains changes in temperature #Call a summary of the model summary(temp_model) The adjusted R squared value (one measure of model fit) is 0.3596. Furthermore the coefficient of our solar_exposure variable is statistically significant. 3.4 Compare the predicted values with the actual values We can use this lm function to predict values of temperature based on the level of solar exposure. We can then compare this to the actual temperature record, and see how well the model fits the data set. #Use this lm model to predict the values MEL_weather_2019 &lt;- MEL_weather_2019 %&gt;% mutate(predicted_temp=predict(temp_model,newdata=MEL_weather_2019)) #Calculate the prediction interval prediction_interval &lt;- predict(temp_model, newdata=MEL_weather_2019, interval = &quot;prediction&quot;) summary(prediction_interval) #Bind this prediction interval data back to the main set MEL_weather_2019 &lt;- cbind(MEL_weather_2019,prediction_interval) MEL_weather_2019 Model fit is easier to interpret graphically. Lets plot the data with the model overlaid. #Plot a chart with data and model on it MEL_temp_predicted &lt;- ggplot(MEL_weather_2019)+ geom_point(aes(y=Max_temp, x=Solar_exposure), col=&quot;grey&quot;)+ geom_line(aes(y=predicted_temp,x=Solar_exposure), col=&quot;blue&quot;)+ geom_smooth(aes(y=Max_temp, x= Solar_exposure), method=lm)+ geom_line(aes(y=lwr,x=Solar_exposure), colour=&quot;red&quot;, linetype=&quot;dashed&quot;)+ geom_line(aes(y=upr,x=Solar_exposure), colour=&quot;red&quot;, linetype=&quot;dashed&quot;)+ labs(title = &quot;Does solar exposure drive temperature in Melbourne?&quot;, subtitle = &#39;Investigation using linear regression&#39;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlab(&quot;Solar exposure&quot;)+ ylab(&quot;Maximum temperature °C&quot;)+ scale_x_continuous(expand=c(0,0), breaks=c(0,5,10,15,20,25,30,35,40))+ theme_bw()+ theme(axis.text=element_text(size=10))+ theme(panel.grid.minor = element_blank()) MEL_temp_predicted This chart includes the model (blue line), confidence interval (grey band around the blue line), and a prediction interval (red dotted line). A prediction interval reflects the uncertainty around a single value (put simple: what is the reasonable upper and lower bound that this data point could be estimated at?). A confidence interval reflects the uncertainty around the mean prediction values (put simply: what is a reasonable upper and lower bound for the blue line at this x value?). Therefore, a prediction interval will be generally much wider than a confidence interval for the same value. 3.5 Analyse the residuals #Add the residuals to the series residuals_temp_predict &lt;- MEL_weather_2019 %&gt;% add_residuals(temp_model) Plot these residuals in a chart. residuals_temp_predict_chart &lt;- ggplot(data=residuals_temp_predict, aes(x=Solar_exposure, y=resid), col=&quot;grey&quot;)+ geom_ref_line(h=0,colour=&quot;blue&quot;, size=1)+ geom_point(col=&quot;grey&quot;)+ xlab(&quot;Solar exposure&quot;)+ ylab(&quot;Maximum temperature (°C)&quot;)+ theme_bw() + labs(title = &quot;Residual values from the linear model&quot;)+ theme(axis.text=element_text(size=12))+ scale_x_continuous(expand=c(0,0)) residuals_temp_predict_chart 3.6 Linear regression with more than one variable The linear model above is *okay*, but can we make it better? Lets start by adding in some more variables into the linear regression. Rainfall data might assist our model in predicting temperature. Lets add in that variable and analyse the results. temp_model_2 &lt;- lm(Max_temp ~ Solar_exposure + Rainfall, data=MEL_weather_2019) summary(temp_model_2) We can see that adding in rainfall made the model better (R squared value has increased to 0.4338). Next, we consider whether solar exposure and rainfall might be related to each other, as well as to temperature. For our third temperature model, we add an interaction variable between solar exposure and rainfall. temp_model_3 &lt;- lm(Max_temp ~ Solar_exposure + Rainfall + Solar_exposure:Rainfall, data=MEL_weather_2019) summary(temp_model_3) We now see this variable is significant, and improves the model slightly (seen by an adjusted R squared of 0.4529). 3.7 Fitting a polynomial regression When analysing the above data set, we see the issue is the sheer variance of temperatures associated with every other variable (it turns out weather forecasting is notoriously difficult). However we can expect that temperature follows a non-linear pattern throughout the year (in Australia it is hot in January-March, cold in June-August, then starts to warm up again). A linear model (e.g. a straight line) will be a very bad model for temperature  we need to introduce polynomials. For simplicity, we will introduce a new variable (Day_number) which is the day of the year (e.g. 1 January is #1, 31 December is #366). MEL_weather_2019 &lt;- MEL_weather_2019 %&gt;% mutate(Day_number=row_number()) head(MEL_weather_2019) Using the same dataset as above, lets plot temperature in Melbourne in 2019. MEL_temp_chart &lt;- ggplot(MEL_weather_2019)+ geom_line(aes(x = Day_number, y = Max_temp)) + labs(title = &#39;Melbourne temperature profile&#39;, subtitle = &#39;Daily maximum temperature recorded in Melbourne in 2019&#39;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlab(&quot;Day of the year&quot;)+ ylab(&quot;Temperature&quot;)+ theme_bw() MEL_temp_chart We can see well need a non-linear model to fit this data. Below we create a few different models. We start with a normal straight line model, then add an x² and x³ model. We then use these models and the predict function to see what temperatures they forecast based on the input data. #Create a straight line estimate to fit the data poly1 &lt;- lm(Max_temp ~ poly(Day_number,1,raw=TRUE), data=MEL_weather_2019) summary(poly1) #Create a polynominal of order 2 to fit this data poly2 &lt;- lm(Max_temp ~ poly(Day_number,2,raw=TRUE), data=MEL_weather_2019) summary(poly2) #Create a polynominal of order 3 to fit this data poly3 &lt;- lm(Max_temp ~ poly(Day_number,3,raw=TRUE), data=MEL_weather_2019) summary(poly3) #Use these models to predict MEL_weather_2019 &lt;- MEL_weather_2019 %&gt;% mutate(poly1values=predict(poly1,newdata=MEL_weather_2019))%&gt;% mutate(poly2values=predict(poly2,newdata=MEL_weather_2019))%&gt;% mutate(poly3values=predict(poly3,newdata=MEL_weather_2019)) head(MEL_weather_2019) In the table above we can see the estimates for that data point from the various models. To see how well the models did graphically, we can plot the original data series with the polynominal models overlaid. #Plot a chart with all models on it MEL_weather_model_chart &lt;- ggplot(MEL_weather_2019)+ geom_line(aes(x=Day_number, y= Max_temp),col=&quot;grey&quot;)+ geom_line(aes(x=Day_number, y= poly1values),col=&quot;red&quot;) + geom_line(aes(x=Day_number, y= poly2values),col=&quot;green&quot;)+ geom_line(aes(x=Day_number, y= poly3values),col=&quot;blue&quot;)+ #Add text annotations geom_text(x=10,y=18,label=&quot;data series&quot;,col=&quot;grey&quot;,hjust=0)+ geom_text(x=10,y=16,label=&quot;linear&quot;,col=&quot;red&quot;,hjust=0)+ geom_text(x=10,y=13,label=parse(text=&quot;x^2&quot;),col=&quot;green&quot;,hjust=0)+ geom_text(x=10,y=10,label=parse(text=&quot;x^3&quot;),col=&quot;blue&quot;,hjust=0)+ labs(title = &quot;Estimating Melbourne&#39;s temperature&quot;, subtitle = &#39;Daily maximum temperature recorded in Melbourne in 2019&#39;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlim(0,366)+ ylim(10,45)+ scale_x_continuous(breaks= c(15,45,75,105,135,165,195,225,255,285,315,345), labels=c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;), expand=c(0,0), limits=c(0,366)) + scale_y_continuous(breaks=c(10,15,20,25,30,35,40,45)) + xlab(&quot;&quot;)+ ylab(&quot;°C&quot;)+ theme_bw()+ theme(axis.text=element_text(size=12))+ theme(panel.grid.minor = element_blank()) MEL_weather_model_chart We can see in the chart above the polynomial models do much better at fitting the data. However, they are still highly variant. Just how variant are they? We can look at the residuals to find out. The residuals is the gap between the observed data point (i.e. the grey line) and our model. #Get the residuals for poly1 residuals_poly1 &lt;- MEL_weather_2019 %&gt;% add_residuals(poly1) residuals_poly1_chart &lt;- ggplot(data=residuals_poly1,aes(x=Day_number, y=resid))+ geom_ref_line(h=0,colour=&quot;red&quot;, size=1)+ geom_line()+ xlab(&quot;&quot;)+ ylab(&quot;°C&quot;)+ theme_bw()+ theme(axis.text=element_text(size=12))+ theme(axis.ticks.x=element_blank(), axis.text.x=element_blank()) residuals_poly1_chart #Get the residuals for poly2 residuals_poly2 &lt;- MEL_weather_2019%&gt;% add_residuals(poly2) residuals_poly2_chart &lt;- ggplot(data=residuals_poly2,aes(x=Day_number, y=resid))+ geom_ref_line(h=0,colour=&quot;green&quot;, size=1)+ geom_line()+ xlab(&quot;&quot;)+ ylab(&quot;°C&quot;)+ theme_bw()+ theme(axis.text=element_text(size=12))+ theme(axis.ticks.x=element_blank(), axis.text.x=element_blank()) residuals_poly2_chart #Get the residuals for poly3 residuals_poly3 &lt;- MEL_weather_2019 %&gt;% add_residuals(poly3) residuals_poly3_chart &lt;- ggplot(data=residuals_poly3,aes(x=Day_number, y=resid))+ geom_ref_line(h=0,colour=&quot;blue&quot;, size=1)+ geom_line()+ theme_bw()+ theme(axis.text=element_text(size=12))+ scale_x_continuous(breaks= c(15,45,75,105,135,165,195,225,255,285,315,345), labels=c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;), expand=c(0,0), limits=c(0,366))+ xlab(&quot;&quot;)+ ylab(&quot;°C&quot;) residuals_poly3_chart three_charts_single_page &lt;- plot_grid( residuals_poly1_chart, residuals_poly2_chart, residuals_poly3_chart, ncol=1,nrow=3,label_size=16) three_charts_single_page As we move from a linear, to a x², to a x³ model, we see the residuals decrease in volatility. "],["working-with-raster-data-in-r.html", "Chapter 4 Working with raster data in R 4.1 Getting started 4.2 Import data 4.3 Data Wrangling 4.4 Working with raster data 4.5 Making an interactive map", " Chapter 4 Working with raster data in R Raster data (sometimes referred to as gridded data) is a type of spatial data that is stored in a grid rather than a polygon. Imagine a chessboard of individuals squares covering the Australian landmass compared to 8 different shapes covering each state and territory. The nice thing about gridded data is that all the cells in the grid are the same size - making calculations much easier. 4.1 Getting started First up we need to load some spatial and data crunching packages. #Load packages library(ncdf4) library(raster) library(rgdal) library(ggplot2) library(sp) library(rgdal) library(absmapsdata) library(dplyr) library(rasterVis) library(RColorBrewer) library(ggplot2) library(viridis) library(sf) library(plyr) library(vctrs) library(tidyr) library(tmap) library(ggmap) library(dplyr) library(ggspatial) library(rlang) library(broom) library(tidyverse) 4.2 Import data Next, we want to import annual rainfall data from github (original source available from the Bureau of Meterology) rainfall &lt;- raster(&quot;https://raw.github.com/charlescoverdale/ERF/master/rainan.txt&quot;) plot(rainfall) Straight away we see this is for the whole of Australia (and then some). Were only interested in whats going on in QLD so lets crop the data down to scale. For this well need to import a shapefile for QLD. The easiest way to do this is using the absmapsdata package - importing a shapefile of Australia then filtering for only Queensland. # Import a polygon for the state of Queensland QLD_shape &lt;- state2016 %&gt;% filter(state_name_2016==&quot;Queensland&quot;) 4.3 Data Wrangling We have raster data for the entirety of Australia (and then some as its pulled from one of the BOMs satellites). This is a bit messy to work with - so lets crop the rainfall data from the entire Australian continent to just Queensland. #Crop data r2 &lt;- crop(rainfall,extent(QLD_shape)) r3 &lt;- mask(r2,QLD_shape) plot(r3) Great. That looks like it worked well. Next up, lets transform the cropped raster (i.e. gridded data) into a data frame (df) that we can use in the ggplot package. r3_df &lt;- as.data.frame(r3,xy=TRUE) r3_df &lt;- r3_df %&gt;% filter(rainan!=&quot;NA&quot;) ggplot() + geom_tile(data=r3_df, aes(x=x, y=y, fill=rainan)) + scale_fill_viridis() + coord_equal() + theme(legend.position=&quot;bottom&quot;) + theme(legend.key.width=unit(1.2, &quot;cm&quot;))+ labs(title=&quot;Rainfall in QLD&quot;, subtitle = &quot;Analysis from the Bureau of Meterology&quot;, caption = &quot;Data: BOM 2021&quot;, x=&quot;&quot;, y=&quot;&quot;, fill=&quot;(mm)&quot;) + theme_minimal() + theme(axis.ticks.x = element_blank(),axis.text.x = element_blank())+ theme(axis.ticks.y = element_blank(),axis.text.y = element_blank())+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ #theme(legend.position = &quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8)) Excellent, weve got a working map of rainfall in Queensland using the ggplot package. Well tidy up the map also and add a title and some better colours. 4.4 Working with raster data For our first piece of data analysis, were going to look at areas with less than 600mm of annual rainfall. How many of our data points will have less than 600mm of rain? Lets take a look at the data distribution and find out. ggplot(r3_df) + geom_histogram(aes(rainan),binwidth=1,col=&quot;darkblue&quot;)+ labs(title=&quot;Distribution of annual rainfall in QLD&quot;, subtitle = &quot;Data using a 5x5km grid&quot;, caption = &quot;Data: Bureau of Meterology 2021&quot;, x=&quot;Rainfall (mm)&quot;, y=&quot;&quot;, fill=&quot;(mm)&quot;) + theme_minimal() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ theme(legend.position = &quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8)) Interesting. The data is heavily right tailed skewed (the mean will be much higher than the median) and most of the data looks to be between 0-1000mm (this makes sense). Lets create a flag column of 0s and 1s that shows when a data point is less than 600mm. r3_df &lt;- r3_df %&gt;% mutate(flag_600mm = ifelse(rainan&lt;=600,1,0)) flagcolours&lt;-(c(&quot;grey&quot;, &quot;#2FB300&quot;)) ggplot() + geom_tile(data=r3_df, aes(x=x, y=y, fill=as.factor(flag_600mm))) + scale_fill_manual(values=flagcolours)+ coord_equal() + theme(legend.position=&quot;bottom&quot;) + theme(legend.key.width=unit(1.2, &quot;cm&quot;))+ labs(title=&quot;Areas with less than 600mm of annual rainfall in QLD&quot;, subtitle = &quot;Identifying suitable land parcels for ERF plantings&quot;, caption = &quot;Data: Bureau of Meterology 2021&quot;, x=&quot;&quot;, y=&quot;&quot;, fill=&quot;(mm)&quot;) + theme_minimal() + theme(axis.ticks.x = element_blank(),axis.text.x = element_blank())+ theme(axis.ticks.y = element_blank(),axis.text.y = element_blank())+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ theme(legend.position = &quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8)) #ggsave(&quot;C:/06 R_code/ERF_600_mm_rainfall.png&quot;,units=&quot;cm&quot;,dpi=200, device=&quot;png&quot;) 4.5 Making an interactive map The static map above is good but an interactive visual (ideally with place names below) is better. To to this well need to convert our data frame back to a raster layer, then plot it using the tmap package. # r4_df &lt;- subset(r3_df,select= -c(rainan)) # # coordinates(r4_df) &lt;- ~ x + y # gridded(r4_df) &lt;- TRUE # r5 &lt;- raster(r4_df) # # tmap_mode(&quot;view&quot;) # tm_shape(r5)+ # tm_raster(style=&quot;cont&quot;,alpha=0.5) "],["election-data-in-r.html", "Chapter 5 Election data in R 5.1 Getting started 5.2 Working with election maps 5.3 Answering election questions 5.4 Demographic analysis of voting trends 5.5 Mapping booths 5.6 Exploring booth level data 5.7 Donkeys, dicks, and other informalities", " Chapter 5 Election data in R Elections tend to create fascinating data sets. They are spatial in nature, comparable over time (i.e. the number of electorates roughly stays the same) - and more importantly they are consequential for all Australians. Australias compulsory voting system is a remarkable feature of our Federation. Every three-ish years we all turn out at over 7,000 polling booths our local schools, churches, and community centres to cast a ballot and pick up an obligatory election day sausage. The byproduct is a fascinating longitudinal and spatial data set. The following code explores different R packages, election data sets, and statistical processes aimed at exploring and modelling federal elections in Australia. One word of warning: I use the term electorates, divisions, and seats interchangeably throughout this chapter. 5.1 Getting started Lets load up some packages #Load packages library(ggparliament) library(eechidna) library(dplyr) library(ggplot2) library(readxl) library(tidyr) library(tidyverse) library(purrr) library(knitr) library(broom) library(absmapsdata) library(sf) library(tmap) library(rmarkdown) library(bookdown) Some phenomenal Australia economists and statisticians have put together a handy election package called eechidna. It includes three main data sets for the most recent Australia federal election (2019). fp19: first preference votes for candidates at each electorate tpp19: two party preferred votes for candidates at each electorate tcp19: two candidate preferred votes for candidates at each electorate Theyve also gone to the trouble of aggregating some census data to the electorate level. This can be found with the abs2016 function. data(fp19) data(tpp19) data(tcp19) data(abs2016) # Show the first few rows #head(tpp16) %&gt;% kable(&quot;simple&quot;) #head(tcp16) %&gt;% kable(&quot;simple&quot;) DT::datatable(tpp19) DT::datatable(tcp19) 5.2 Working with election maps As noted in the introduction, elections are spatial in nature. Not only does geography largely determine policy decisions, we see that many electorates vote for the same party (or even the same candidate) for decades. How electorate boundaries are drawn is a long story (see here, here, and here). The summary version is the AEC carves up the population by state and territory, uses a wacky formula to decide how many seats each state and territory should be allocated, then draws maps to try and get a roughly equal number of people in each electorate. Oh and did I mention for reasons that arent worth explaining that Tasmania has to have at least 5 seats? Our Federation is a funny thing. Anyhow, at time of writing this is how the breakdown of seats looks. State/Territory Number of members of the House of Representatives New South Wales 47 Victoria 39 Queensland 30 Western Australia 15 South Australia 10 Tasmania 5 Australian Capital Territory 3 Northern Territory 2* TOTAL 151 Note: The NT doesnt have the population to justify its second seat . The AEC scheduled to dissolve it after the 2019 election but Parliament intervened in late 2020 and a bill was passed to make sure both seats were kept (creating 151 nationally). How variant are these 151 electorates in size? Massive. Durack in Western Australia (1.63 million square kilometres) is by far the largest and the smallest is Grayndler in New South Wales (32 square kilometres). Lets make a map to make things easier. CED_map &lt;- ced2018 %&gt;% ggplot()+ geom_sf()+ labs(title=&quot;Electoral divisions in Australia&quot;, subtitle = &quot;It turns out we divide the country in very non-standard blocks&quot;, caption = &quot;Data: Australian Bureau of Statistics 2016&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(axis.ticks.x = element_blank(),axis.text.x = element_blank())+ theme(axis.ticks.y = element_blank(),axis.text.y = element_blank())+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ theme(legend.position = &quot;right&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8)) CED_map_remove_6 &lt;- ced2018 %&gt;% dplyr::filter(!ced_code_2018 %in% c(506,701,404,511,321,317)) %&gt;% ggplot()+ geom_sf()+ labs(title=&quot;194 electoral divisions in Australia&quot;, subtitle = &quot;Turns out removing the largest 6 electorates makes a difference&quot;, caption = &quot;Data: Australian Bureau of Statistics 2016&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(axis.ticks.x = element_blank(),axis.text.x = element_blank())+ theme(axis.ticks.y = element_blank(),axis.text.y = element_blank())+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ theme(legend.position = &quot;right&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8)) CED_map CED_map_remove_6 Next lets look at what party/candidate is currently the sitting member for each electorate. To do this on a map were going to need to join our tcp19 data and the ced2018 spatial data. In the first data set, the electorate column in called DivisionNm and in the second ced_name_2018. We see the data in our DivisionNm variable is in UPPERCASE while our ced_name_2018 variable is in Titlecase. Lets change the first variable to Titlecase. We can then make the column names the same, and run our left_join function. #Pull in the electorate shapefiles from the absmapsdata package electorates &lt;- ced2018 #Make the DivisionNm Titlecase tcp19$DivisionNm=str_to_title(tcp19$DivisionNm) tcp19_edit &lt;- tcp19 %&gt;% distinct() %&gt;% filter(Elected == &quot;Y&quot;) #Make the column names the same electorates &lt;- dplyr::rename(electorates, DivisionNm = ced_name_2018) ced_map_data &lt;- left_join(tcp19_edit, electorates, by = &quot;DivisionNm&quot;) ced_map_data &lt;- as.data.frame(ced_map_data) head(ced_map_data) str(ced_map_data) ggplot()+ geom_sf(data=ced_map_data,aes(geometry = geometry,fill=PartyAb)) + theme_minimal() + theme(axis.ticks.x = element_blank(),axis.text.x = element_blank())+ theme(axis.ticks.y = element_blank(),axis.text.y = element_blank())+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ theme(legend.position = &quot;right&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ scale_color_manual(&quot;PartyAb&quot;, values=c(&quot;LP&quot; =&quot;#80b1d3&quot;, &quot;NP&quot; = &quot;#006400&quot;, &quot;ALP&quot;= &quot;#fb8072&quot;, &quot;GVIC&quot; = &quot;#33a02c&quot;, &quot;XEN&quot; = &quot;#beaed4&quot;, &quot;ON&quot; = &quot;#fdc086&quot;, &quot;KAP&quot; = &quot;#ffff99&quot;, &quot;IND&quot; = &quot;grey25&quot;)) 5.3 Answering election questions Lets start by answering a simple question: who won the election? For this well need to use the two-candidate preferred data set (to make sure we capture all the minor parties that won seats). who_won &lt;- tcp19 %&gt;% filter(Elected == &quot;Y&quot;) %&gt;% group_by(PartyNm) %&gt;% tally() %&gt;% arrange(desc(n)) # inspect who_won %&gt;% kable(&quot;simple&quot;) PartyNm n AUSTRALIAN LABOR PARTY 68 LIBERAL PARTY 67 NATIONAL PARTY 10 INDEPENDENT 3 CENTRE ALLIANCE 1 KATTERS AUSTRALIAN PARTY (KAP) 1 THE GREENS 1 Next up lets see which candidates won with the smallest percentage of votes who_won_least_votes_prop &lt;- fp16 %&gt;% filter(Elected == &quot;Y&quot;) %&gt;% arrange(Percent) %&gt;% mutate(candidate_full_name = paste0(GivenNm, &quot; &quot;, Surname, &quot; (&quot;, CandidateID, &quot;)&quot;)) %&gt;% dplyr::select(candidate_full_name, PartyNm, DivisionNm, Percent) who_won_least_votes_prop %&gt;% head %&gt;% kable(&quot;simple&quot;) candidate_full_name PartyNm DivisionNm Percent MICHAEL DANBY (28267) AUSTRALIAN LABOR PARTY MELBOURNE PORTS 27.00 CATHY OTOOLE (28997) AUSTRALIAN LABOR PARTY HERBERT 30.45 JUSTINE ELLIOT (28987) AUSTRALIAN LABOR PARTY RICHMOND 31.05 TERRI BUTLER (28921) AUSTRALIAN LABOR PARTY GRIFFITH 33.18 STEVE GEORGANAS (29071) AUSTRALIAN LABOR PARTY HINDMARSH 34.02 CATHY MCGOWAN (23288) INDEPENDENT INDI 34.76 This is really something. The relationship were seeing here seems to be these are the seats in which the ALP relies heavily on preference flows from the Greens or Independents to win. The electorate I grew up in is listed here (Richmond) - lets look at how the votes were allocated. Richmond_fp &lt;- fp16 %&gt;% filter(DivisionNm == &quot;RICHMOND&quot;) %&gt;% arrange(-Percent) %&gt;% mutate(candidate_full_name = paste0(GivenNm, &quot; &quot;, Surname, &quot; (&quot;, CandidateID, &quot;)&quot;)) %&gt;% dplyr::select(candidate_full_name, PartyNm, DivisionNm, Percent, OrdinaryVotes) Richmond_fp %&gt;% knitr::kable(&quot;simple&quot;) candidate_full_name PartyNm DivisionNm Percent OrdinaryVotes MATTHEW FRASER (29295) NATIONAL PARTY RICHMOND 37.61 37006 JUSTINE ELLIOT (28987) AUSTRALIAN LABOR PARTY RICHMOND 31.05 30551 DAWN WALKER (28783) THE GREENS RICHMOND 20.44 20108 NEIL GORDON SMITH (28349) ONE NATION RICHMOND 6.26 6160 ANGELA POLLARD (29290) ANIMAL JUSTICE PARTY RICHMOND 3.14 3089 RUSSELL KILARNEY (28785) CHRISTIAN DEMOCRATIC PARTY RICHMOND 1.51 1484 Sure enough - the Greens certainly helped get the ALP across the line. The interpretation that these seats are the most marginal is incorrect (e.g. imagine if ALP win 30% and the Greens win 30% - that is a pretty safe 10% margin assuming traditional preference flows). But - lets investigate which seats are the most marginal. who_won_smallest_margin &lt;- tcp16 %&gt;% filter(Elected == &quot;Y&quot;) %&gt;% mutate(percent_margin = 2*(Percent - 50), vote_margin = round(percent_margin * OrdinaryVotes / Percent)) %&gt;% arrange(Percent) %&gt;% mutate(candidate_full_name = paste0(GivenNm, &quot; &quot;, Surname, &quot; (&quot;, CandidateID, &quot;)&quot;)) %&gt;% dplyr::select(candidate_full_name, PartyNm, DivisionNm, Percent, OrdinaryVotes, percent_margin, vote_margin) # have a look who_won_smallest_margin %&gt;% head %&gt;% knitr::kable(&quot;simple&quot;) candidate_full_name PartyNm DivisionNm Percent OrdinaryVotes percent_margin vote_margin CATHY OTOOLE (28997) AUSTRALIAN LABOR PARTY HERBERT 50.02 44187 0.04 35 STEVE GEORGANAS (29071) AUSTRALIAN LABOR PARTY HINDMARSH 50.58 49586 1.16 1137 MICHELLE LANDRY (28034) LIBERAL PARTY CAPRICORNIA 50.63 44633 1.26 1111 BERT VAN MANEN (28039) LIBERAL PARTY FORDE 50.63 42486 1.26 1057 ANNE ALY (28727) AUSTRALIAN LABOR PARTY COWAN 50.68 41301 1.36 1108 ANN SUDMALIS (28668) LIBERAL PARTY GILMORE 50.73 52336 1.46 1506 Crikey. We see Cathy OToole got in with a 0.04% margin (just 35 votes!) While were at it we better do the opposite and see who romped it by the largest margin. who_won_largest_margin &lt;- tcp16 %&gt;% filter(Elected == &quot;Y&quot;) %&gt;% mutate(percent_margin = 2*(Percent - 50), vote_margin = round(percent_margin * OrdinaryVotes / Percent)) %&gt;% arrange(desc(Percent)) %&gt;% mutate(candidate_full_name = paste0(GivenNm, &quot; &quot;, Surname, &quot; (&quot;, CandidateID, &quot;)&quot;)) %&gt;% dplyr::select(candidate_full_name, PartyNm, DivisionNm, Percent, OrdinaryVotes, percent_margin, vote_margin) # Look at the data who_won_largest_margin %&gt;% head %&gt;% knitr::kable(&quot;simple&quot;) candidate_full_name PartyNm DivisionNm Percent OrdinaryVotes percent_margin vote_margin ANDREW BROAD (28415) NATIONAL PARTY MALLEE 71.32 62383 42.64 37297 PAUL FLETCHER (28565) LIBERAL PARTY BRADFIELD 71.04 66513 42.08 39398 JULIE BISHOP (28746) LIBERAL PARTY CURTIN 70.70 60631 41.40 35504 SUSSAN LEY (28699) LIBERAL PARTY FARRER 70.53 68114 41.06 39653 JASON CLARE (28931) AUSTRALIAN LABOR PARTY BLAXLAND 69.48 55507 38.96 31125 BRENDAN OCONNOR (28274) AUSTRALIAN LABOR PARTY GORTON 69.45 68135 38.90 38163 Wowza. Thats really something. Some candidates won seats with a 30-40 percent margin - scooping up 70% of the two candidate preferred vote in the process! who_won &lt;- tcp16 %&gt;% filter(Elected == &quot;Y&quot;) %&gt;% group_by(PartyNm, StateAb) %&gt;% tally() %&gt;% arrange(desc(n)) who_won_by_state &lt;- spread(who_won,StateAb, n) %&gt;% arrange(desc(NSW)) #View data set who_won_by_state %&gt;% knitr::kable(&quot;simple&quot;) 5.4 Demographic analysis of voting trends Now weve figured out how to work with election data - lets link it up to some AUstralian demographic data. The eechidna package includes a cleaned set of census data from 2016 that has already been adjusted from ASGS boundaries to Commonwealth Electoral Divisions. # Import the census data from the eechidna package data(eechidna::abs2016) head(abs2016) # Join with two-party preferred voting data data(tpp10) election2016 &lt;- left_join(abs2016, tpp10, by = &quot;DivisionNm&quot;) Thats what we want to see. 150 rows of data (one for each electorate) and over 80 columns of census variables. A starting exploratory exercise is too see which of these variables are correlated with voting for one party or another. Theres some old narrative around LNP voters being rich, old, white, and somehow upper class compared to the population at large. Lets pick a few variables that roughly match with this criteria (Income, Age, English language speakers, and Bachelor educated) and chart it compared to LNP percentage of the vote. # See relationship between personal income and Liberal/National support ggplot(election2016, aes(x = MedianPersonalIncome, y = LNP_Percent)) + geom_point() + geom_smooth() ggplot(election2016, aes(x = MedianAge, y = LNP_Percent)) + geom_jitter() + geom_smooth() ggplot(election2016, aes(x = EnglishOnly, y = LNP_Percent)) + geom_jitter() + geom_smooth() ggplot(election2016, aes(x = BachelorAbv, y = LNP_Percent)) + geom_jitter() + geom_smooth() First impressions: Geez this data looks messy. Second impression: Maybe theres a bit of a trend with age and income? Lets build a regression model to run all the 80 odd census variables in the abs2016 data set against the LNP_percent variable. # We can use colnames(election2016) to get a big list of all the variables available # Now we build the model election_model &lt;- lm(LNP_Percent~ Population+ Area+ Age00_04+ Age05_14+ Age15_19+ Age20_24+ Age25_34+ Age35_44+ Age45_54+ Age55_64+ Age65_74+ Age75_84+ Age85plus+ Anglican+ AusCitizen+ AverageHouseholdSize+ BachelorAbv+Born_Asia+ Born_MidEast+Born_SE_Europe+ Born_UK+ BornElsewhere+ Buddhism+ Catholic+ Christianity+ Couple_NoChild_House+Couple_WChild_House+ CurrentlyStudying+DeFacto+ DiffAddress+ DipCert+ EnglishOnly+ FamilyRatio+ Finance+ HighSchool+ Indigenous+ InternetAccess+ InternetUse+ Islam+ Judaism+ Laborer+ LFParticipation+ Married+ MedianAge+ MedianFamilyIncome+ MedianHouseholdIncome+ MedianLoanPay+ MedianPersonalIncome+ MedianRent+ Mortgage+ NoReligion+ OneParent_House+ Owned+ Professional+ PublicHousing+ Renting+ SocialServ+ SP_House+ Tradesperson+ Unemployed+ Volunteer, data=election2016) summary(election_model) ## ## Call: ## lm(formula = LNP_Percent ~ Population + Area + Age00_04 + Age05_14 + ## Age15_19 + Age20_24 + Age25_34 + Age35_44 + Age45_54 + Age55_64 + ## Age65_74 + Age75_84 + Age85plus + Anglican + AusCitizen + ## AverageHouseholdSize + BachelorAbv + Born_Asia + Born_MidEast + ## Born_SE_Europe + Born_UK + BornElsewhere + Buddhism + Catholic + ## Christianity + Couple_NoChild_House + Couple_WChild_House + ## CurrentlyStudying + DeFacto + DiffAddress + DipCert + EnglishOnly + ## FamilyRatio + Finance + HighSchool + Indigenous + InternetAccess + ## InternetUse + Islam + Judaism + Laborer + LFParticipation + ## Married + MedianAge + MedianFamilyIncome + MedianHouseholdIncome + ## MedianLoanPay + MedianPersonalIncome + MedianRent + Mortgage + ## NoReligion + OneParent_House + Owned + Professional + PublicHousing + ## Renting + SocialServ + SP_House + Tradesperson + Unemployed + ## Volunteer, data = election2016) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.6197 -2.5288 -0.2903 2.2118 10.0752 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.602e+03 1.198e+04 -0.634 0.52753 ## Population 4.224e-05 5.045e-05 0.837 0.40468 ## Area 6.359e-06 5.436e-06 1.170 0.24527 ## Age00_04 7.063e+01 1.193e+02 0.592 0.55554 ## Age05_14 7.280e+01 1.194e+02 0.609 0.54380 ## Age15_19 7.215e+01 1.193e+02 0.605 0.54686 ## Age20_24 6.383e+01 1.198e+02 0.533 0.59559 ## Age25_34 7.265e+01 1.196e+02 0.607 0.54522 ## Age35_44 6.830e+01 1.195e+02 0.572 0.56912 ## Age45_54 6.964e+01 1.196e+02 0.582 0.56192 ## Age55_64 7.224e+01 1.194e+02 0.605 0.54677 ## Age65_74 7.915e+01 1.197e+02 0.661 0.51017 ## Age75_84 7.583e+01 1.193e+02 0.635 0.52682 ## Age85plus 6.974e+01 1.197e+02 0.582 0.56178 ## Anglican 2.912e-01 3.954e-01 0.737 0.46338 ## AusCitizen 1.679e-01 7.769e-01 0.216 0.82938 ## AverageHouseholdSize -2.683e+01 1.673e+01 -1.604 0.11241 ## BachelorAbv -3.045e+00 1.003e+00 -3.035 0.00318 ** ## Born_Asia -3.451e-01 3.842e-01 -0.898 0.37151 ## Born_MidEast 8.345e-01 1.256e+00 0.664 0.50832 ## Born_SE_Europe -2.007e+00 1.479e+00 -1.357 0.17825 ## Born_UK 6.477e-02 4.615e-01 0.140 0.88872 ## BornElsewhere 6.542e-01 6.265e-01 1.044 0.29933 ## Buddhism 6.375e-01 8.253e-01 0.772 0.44196 ## Catholic -3.970e-01 3.754e-01 -1.058 0.29322 ## Christianity 1.112e+00 6.283e-01 1.769 0.08038 . ## Couple_NoChild_House 3.345e+00 3.078e+00 1.087 0.28021 ## Couple_WChild_House 3.762e+00 3.156e+00 1.192 0.23661 ## CurrentlyStudying 2.597e+00 1.303e+00 1.993 0.04939 * ## DeFacto -7.035e+00 2.617e+00 -2.688 0.00862 ** ## DiffAddress 9.320e-01 3.877e-01 2.404 0.01836 * ## DipCert -8.790e-01 7.296e-01 -1.205 0.23163 ## EnglishOnly -1.316e-01 4.717e-01 -0.279 0.78095 ## FamilyRatio 1.962e+01 4.994e+01 0.393 0.69543 ## Finance 1.528e+00 9.060e-01 1.687 0.09523 . ## HighSchool 9.371e-01 4.683e-01 2.001 0.04854 * ## Indigenous 1.054e+00 4.781e-01 2.205 0.03013 * ## InternetAccess -9.364e-01 9.367e-01 -1.000 0.32028 ## InternetUse NA NA NA NA ## Islam 2.894e-01 6.407e-01 0.452 0.65258 ## Judaism 7.306e-01 7.474e-01 0.978 0.33105 ## Laborer -2.925e-02 7.905e-01 -0.037 0.97057 ## LFParticipation 2.926e+00 8.800e-01 3.325 0.00130 ** ## Married -4.168e+00 1.890e+00 -2.205 0.03011 * ## MedianAge -7.214e-01 1.071e+00 -0.674 0.50218 ## MedianFamilyIncome 2.146e-02 3.339e-02 0.643 0.52204 ## MedianHouseholdIncome 2.879e-02 3.037e-02 0.948 0.34584 ## MedianLoanPay -9.533e-03 1.336e-02 -0.713 0.47757 ## MedianPersonalIncome -2.286e-02 5.907e-02 -0.387 0.69973 ## MedianRent -4.070e-02 5.306e-02 -0.767 0.44514 ## Mortgage 1.923e+00 1.741e+00 1.104 0.27264 ## NoReligion 1.285e+00 6.365e-01 2.019 0.04658 * ## OneParent_House 2.164e-01 2.987e+00 0.072 0.94241 ## Owned 1.488e+00 1.591e+00 0.935 0.35229 ## Professional 7.449e-01 1.010e+00 0.737 0.46300 ## PublicHousing -5.464e-01 6.280e-01 -0.870 0.38670 ## Renting 2.068e+00 1.736e+00 1.192 0.23664 ## SocialServ -3.356e-01 6.232e-01 -0.539 0.59155 ## SP_House -1.034e+00 8.907e-01 -1.161 0.24895 ## Tradesperson 6.347e-01 8.058e-01 0.788 0.43305 ## Unemployed 2.815e-01 1.176e+00 0.239 0.81149 ## Volunteer 7.346e-01 6.179e-01 1.189 0.23772 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.933 on 86 degrees of freedom ## (3 observations deleted due to missingness) ## Multiple R-squared: 0.8911, Adjusted R-squared: 0.8151 ## F-statistic: 11.73 on 60 and 86 DF, p-value: &lt; 2.2e-16 For the people that care about statistical fit and endogenous variables, you may have concerns (and rightly so) with the above approach. Its pretty rough. Lets run a basic check to see if the residuals are normally distributed. hist(election_model$residuals, col=&quot;bisque&quot;, freq=FALSE, main=NA) lines(density(election_model$residuals), col=&quot;red&quot;) Hmm thats actually not too bad. Onwards. We see now that only a handful of these variables in the table above are statistically significant. Running an updated (and leaner) model gives: election_model_lean &lt;- lm(LNP_Percent~ BachelorAbv+ CurrentlyStudying+ DeFacto+ DiffAddress+ Finance+HighSchool+ Indigenous+ LFParticipation+ Married+ NoReligion, data=election2016) summary(election_model_lean) ggplot(election2016, aes(x = BachelorAbv, y = LNP_Percent)) + geom_point() + geom_smooth()+theme_bw() ggplot(election2016, aes(x = CurrentlyStudying, y = LNP_Percent)) + geom_jitter() + geom_smooth()+theme_bw() ggplot(election2016, aes(x = DeFacto, y = LNP_Percent)) + geom_jitter() + geom_smooth()+theme_bw() ggplot(election2016, aes(x = DiffAddress, y = LNP_Percent)) + geom_jitter() + geom_smooth()+theme_bw() ggplot(election2016, aes(x = Finance, y = LNP_Percent)) + geom_jitter() + geom_smooth()+theme_bw() ggplot(election2016, aes(x = HighSchool, y = LNP_Percent)) + geom_jitter() + geom_smooth()+theme_bw() ggplot(election2016, aes(x = Indigenous, y = LNP_Percent)) + geom_jitter() + geom_smooth()+theme_bw() ggplot(election2016, aes(x = LFParticipation, y = LNP_Percent)) + geom_jitter() + geom_smooth()+theme_bw() ggplot(election2016, aes(x = Married, y = LNP_Percent)) + geom_jitter() + geom_smooth()+theme_bw() ggplot(election2016, aes(x = NoReligion, y = LNP_Percent)) + geom_jitter() + geom_smooth()+theme_bw() My main gripe with the above is that electorates are very different in size. Therefore trying to conclude any statistical relationship on an electorate level is prone to errors. Adding more data isnt always the best method to solve whats formally known as the Modifiable Area Unit Problem but in this case its worth a try. So here goes, lets run the analysis above, this time using all 7,000 voting booths (and their local demographic data) as the data set rather than just the 150 electorates. 5.5 Mapping booths The AEC maintains a handy spreadsheet of booth locations for recent federal elections. You can search for your local booth location (probably a school, church, or community center) in the table below. What do these booths look like on a map? Lets reuse the CED map above and plot a point for each booth location. ggplot()+ geom_sf(data=ced2018)+ geom_point(data=booths, aes(x=Longitude, y=Latitude), colour=&quot;purple&quot;, size=1, alpha=0.3, inherit.aes=FALSE) + labs(title=&quot;Polling booths in Australia&quot;, subtitle = &quot; &quot;, caption = &quot;Data: Australian Electoral Comission 2016&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(axis.ticks.x = element_blank(),axis.text.x = element_blank())+ theme(axis.ticks.y = element_blank(),axis.text.y = element_blank())+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ theme(legend.position = &quot;right&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8)) + xlim(c(112,157)) + ylim(c(-44,-11)) 5.6 Exploring booth level data Figuring out where a candidates votes come from within an electorate is fundamental to developing a campaign strategy. Even in small electorates (e.g. Wentworth), there are pockets of right leaning and left leaning districts. Once you factor in preference flows - this multi-variate calculus becomes important to winning or maintaining a seat. In the eechidnapackage, election results are provided at the resolution of polling place. They nmust be downloaded using the functions firstpref_pollingbooth_download, twoparty_pollingbooth_download or twocand_pollingbooth_download (depending on the vote type). The two files need to be merged to be useful for analysis.. Both have a unique ID for the polling place that can be used to match the records. The two party preferred vote, a measure of preference between only the Australian Labor Party (ALP) and the Liberal/National Coalition (LNP), is downloaded using twoparty_pollingbooth_download. The preferred party is the one with the higher percentage, and we use this to colour the points indicating polling places. We see that within some big rural electorates (e.g. in Western NSW), there are pockets of ALP preference despite the seat going to the LNP. Note that this data set is on a tpp basis - so we cant see the booths that were won by minor parties (although it would be fascinating). ## Error in `geom_map()`: ## ! `map` must have the columns `x`, `y`, and `id` The two candidate preferred vote (downloaded with twocand_pollingbooth_download) is a measure of preference between the two candidates who received the most votes through the division of preferences, where the winner has the higher percentage. ## Error in `geom_map()`: ## ! `map` must have the columns `x`, `y`, and `id` 5.7 Donkeys, dicks, and other informalities Were about to go off the deep end into a certain type of election data. In the 2016 Australian Federal Election, over 720,915 people (5.5% of all votes cast) voted informally. Of these, over half (377,585) had no clear first preference, meaning their vote did not contribute to the campaign of any candidate. Ill be honest, informal votes absolutely fascinate me. Not only are there 8 types of informal votes (you can read all about the Australian Electoral Commissions analysis here), but the rate of informal voting varies a tremendous amount by electorate. Broadly, we can think of informal votes in two main buckets. Protest votes Stuff-ups If we want to get particular about it, I like to subcategorise these buckets into: Protest votes (i.e. a person that thinks they are voting against): the democratic system, their local selection of candidates on the ballot, or the two most likely candidates for PM. Stuff ups (people who): filled in the form wrong but a clear preference was still made stuffed up the form entirely and it didnt contribute towards the tally for any candidtate This is the good bit: The AEC works tirelessly to reduce stuff-ups on ballot papers (clear instructions and UI etc), but there isnt much of a solution for protest votes. Whats interesting is you can track the vibe of how consequential an election is by the proportion of protest votes. Lets pull some informal voting data from the AEC website. "],["charts.html", "Chapter 6 Charts 6.1 Getting started 6.2 Make the data tidy 6.3 Line plot 6.4 Scatter and trend plot 6.5 Shading areas on plots 6.6 Bar chart (numercial) 6.7 Stacked bar chart 6.8 Histogram 6.9 Ridge chart 6.10 BBC style: Bar charts (categorical) 6.11 BBC style: Dumbbell charts 6.12 Facet wraps 6.13 Pie chart 6.14 Patchwork 6.15 Saving to powerpoint 6.16 Automating chart creation", " Chapter 6 Charts 6.1 Getting started Theres exceptional resources online for using the ggplot2 package to create production ready charts. The R Graph Gallery is a great place to start, as is the visual storytelling blogs of The Economist and the BBC. This chapter contains the code for some of my most used charts and visualization techniques. # Load in packages library(ggridges) library(ggplot2) library(ggrepel) library(viridis) library(readxl) library(hrbrthemes) library(dplyr) library(stringr) library(reshape) library(tidyr) library(lubridate) library(gapminder) library(ggalt) library(purrr) library(scales) library(purrr) library(patchwork) #library(bbplot) 6.2 Make the data tidy Before making a chart ensure the data is tidy - meaning there is a new row for every changed variable. It also doesnt hurt to remove NAs for consistency (particularly in time series). #Read in data url &lt;-&quot;https://raw.githubusercontent.com/charlescoverdale/ggridges/master/2019_MEL_max_temp_daily.xlsx&quot; #Read in with read.xlsx MEL_temp_daily &lt;- openxlsx::read.xlsx(url) #Remove last 2 characters to just be left with the day number MEL_temp_daily$Day=substr(MEL_temp_daily$Day,1,nchar(MEL_temp_daily$Day)-2) #Make a wide format long using the gather function MEL_temp_daily &lt;- MEL_temp_daily %&gt;% gather(Month,Temp,Jan:Dec) MEL_temp_daily$Month&lt;-factor(MEL_temp_daily$Month,levels=c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;)) #Add in a year MEL_temp_daily[&quot;Year&quot;]=2019 #Reorder MEL_temp_daily &lt;- MEL_temp_daily[,c(1,2,4,3)] #Make a single data field using lubridate MEL_temp_daily &lt;- MEL_temp_daily %&gt;% mutate(Date = make_date(Year, Month, Day)) #Drop the original date columns MEL_temp_daily &lt;- MEL_temp_daily %&gt;% dplyr::select(Date, Temp) %&gt;% drop_na() #Add on a 7-day rolling average MEL_temp_daily &lt;- MEL_temp_daily %&gt;% dplyr::mutate(Seven_day_rolling = zoo::rollmean(Temp, k = 7, fill = NA), Mean = mean(Temp)) #Drop NA&#39;s #MEL_temp_daily &lt;- MEL_temp_daily %&gt;% drop_na() 6.3 Line plot plot_MEL_temp &lt;- ggplot(MEL_temp_daily)+ geom_line(aes(x = Date, y = Temp), col = &quot;blue&quot;)+ geom_line(aes(x = Date, y = Mean), col = &quot;orange&quot;)+ labs(title=&quot;Hot in the summer and cool in the winter&quot;, subtitle = &quot;Analysing temperature in Melbourne&quot;, caption = &quot;Data: Bureau of Meterology 2019&quot;, x=&quot;&quot;, y=&quot;&quot;) + scale_x_date(date_breaks = &quot;1 month&quot;, date_labels = &quot;%b&quot;, limits = as.Date(c(&#39;2019-01-01&#39;,&#39;2019-12-14&#39;)))+ scale_y_continuous(label = unit_format(unit=&quot;\\u00b0C&quot;, sep=&quot;&quot;)) + theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -20)))+ theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4))+ annotate(geom=&#39;curve&#39;, x=as.Date(&#39;2019-08-01&#39;), y=23, xend=as.Date(&#39;2019-08-01&#39;),yend=17, curvature=-0.5,arrow=arrow(length=unit(2,&quot;mm&quot;)))+ annotate(geom=&#39;text&#39;,x=as.Date(&#39;2019-07-15&#39;),y=25, label=&quot;Below 20°C all winter&quot;) plot_MEL_temp 6.4 Scatter and trend plot MEL_temp_Jan &lt;- MEL_temp_daily %&gt;% filter(MEL_temp_daily$Date&lt;as.Date(&quot;2019-01-31&quot;)) ggplot(MEL_temp_Jan)+ geom_point(aes(x = Date, y = Temp), col = &quot;purple&quot;,alpha=0.4)+ geom_smooth(aes(x = Date, y = Temp), col = &quot;purple&quot;,fill=&quot;purple&quot;, alpha=0.1,method = &quot;lm&quot;)+ #scale_colour_manual(values = c(&quot;purple&quot;),labels=&quot;Trend&quot;) + #scale_fill_manual(values = c(&quot;purple&quot;),labels=&quot;Confidence interval&quot;)+ labs(title=&quot;January is a hot one&quot;, subtitle = &quot;Analysing temperature in Melbourne&quot;, caption = &quot;Data: Bureau of Meterology 2019&quot;, x=&quot;&quot;, y=&quot;Temperature °C&quot;) + scale_x_date(date_breaks = &quot;1 week&quot;, date_labels = &quot;%d-%b&quot;, limits = as.Date(c(&#39;2019-01-01&#39;,&#39;2019-01-31&#39;)), expand = c(0,0)) + theme_minimal() + theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(axis.title.y = element_text(size=9,margin=margin(t = 0, r = 10, b = 0, l = 0)))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4))+ geom_hline(yintercept=45,colour =&quot;black&quot;,size=0.4) + annotate(geom=&#39;curve&#39;, x=as.Date(&#39;2019-01-22&#39;), y=37.5, xend=as.Date(&#39;2019-01-25&#39;),yend=42, curvature=0.5, col=&quot;#575757&quot;, arrow=arrow(length=unit(2,&quot;mm&quot;))) + annotate(geom=&#39;text&#39;,x=as.Date(&#39;2019-01-16&#39;),y=37.5, label=&quot;January saw some extreme temperatures&quot;,size=3.2,col=&quot;#575757&quot;) 6.5 Shading areas on plots Adding shading behind a plot area is simple using geom_rect. Adding shading under a particular model line? A little trickier. See both example below. #Example 1 #Set a custom limit for the y-axis threshold threshold &lt;- 20 ggplot() + geom_point(data = mtcars, aes(x = hp, y = mpg)) + #Add an intercept line geom_hline(yintercept = threshold) + # Shade area under y_lim geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = threshold), alpha = 1/5, fill = &quot;blue&quot;) + # Shade area above y_lim geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = threshold, ymax = Inf), alpha = 1/5, fill = &quot;red&quot;) #Example 2 #Now to add colour either way of a model line #Define the model model &lt;- lm(mpg ~ log(hp), data = mtcars) # Get the predictions for plotting. Here, df_line, is a data frame with new # coordinates that will be used for plotting the trend line and further for # building the polygons for shading. min_x &lt;- min(mtcars$hp) max_x &lt;- max(mtcars$hp) df_line &lt;- data.frame(hp = seq(from = min_x, to = max_x, by = 1)) df_line$mpg &lt;- predict(model, newdata = df_line) p &lt;- ggplot() + geom_point(data = mtcars, aes(x = hp, y = mpg),color=&quot;grey&quot;,alpha=0.5) + geom_line(data = df_line, aes(x = hp, y = mpg),col=&quot;black&quot;) #Define two polygons (one above, and one below the line) df_poly_under &lt;- df_line %&gt;% tibble::add_row(hp = c(max_x, min_x), mpg = c(-Inf, -Inf)) df_poly_above &lt;- df_line %&gt;% tibble::add_row(hp = c(max_x, min_x), mpg = c(Inf, Inf)) #Plot the data, line, and the shades above and below the line p + geom_polygon(data = df_poly_under, aes(x = hp, y = mpg), fill = &quot;blue&quot;, alpha = 1/5) + geom_polygon(data = df_poly_above, aes(x = hp, y = mpg), fill = &quot;red&quot;, alpha = 1/5)+ scale_x_discrete(expand = c(0,0))+ theme_minimal() + labs(title=&quot;Look at that snazzy red/blue shaded area&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())+ theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) 6.6 Bar chart (numercial) Year = c(&quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;) Value = (c(1000000,3000000, 2000000, 5000000)) bar_data_single &lt;- (cbind(Year, Value)) bar_data_single &lt;- as.data.frame(bar_data_single) bar_data_single$Value = as.integer(bar_data_single$Value) ggplot(bar_data_single, aes(x = Year, y = Value, label=Value)) + geom_bar(stat=&#39;identity&#39;,fill=&quot;blue&quot;,width=0.8)+ geom_text(size = 5, col=&quot;white&quot;,fontface=&quot;bold&quot;, position = position_stack(vjust = 0.5), label=scales::dollar(Value,scale=1/1e6,suffix=&quot;m&quot;))+ labs(title=&quot;Bar chart example&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=12))+ theme(axis.text=element_text(size=12))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank())+ theme(panel.grid.major.y = element_blank()) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) # ggsave(plot=last_plot(), # width=10, # height=10, # units=&quot;cm&quot;, # dpi = 600, # filename = &quot;/Users/charlescoverdale/Desktop/test.png&quot;) 6.7 Stacked bar chart Year = c(&quot;2019&quot;, &quot;2019&quot;, &quot;2019&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2020&quot;, &quot;2020&quot;, &quot;2020&quot;) Quarter = c(&quot;Q1&quot;,&quot;Q2&quot;,&quot;Q3&quot;, &quot;Q4&quot;,&quot;Q1&quot;,&quot;Q2&quot;,&quot;Q3&quot;, &quot;Q4&quot;) Value = (c(100,300,200,500,400,700,200,300)) bar_data &lt;- (cbind(Year, Quarter, Value)) bar_data &lt;- as.data.frame(bar_data) bar_data$Value = as.integer(bar_data$Value) bar_data_totals &lt;- bar_data %&gt;% dplyr::group_by(Year) %&gt;% dplyr:: summarise(Total = sum(Value)) ggplot(bar_data, aes(x = Year, y = Value, fill = (Quarter), label=Value)) + geom_bar(position = position_stack(reverse=TRUE),stat=&#39;identity&#39;)+ geom_text(size = 4, col=&quot;white&quot;, fontface=&quot;bold&quot;, position = position_stack(reverse=TRUE,vjust = 0.5), label=scales::dollar(Value))+ geom_text(aes(Year, Total, label=scales::dollar(Total), fill = NULL, vjust=-0.5), fontface=&quot;bold&quot;, size=4, data = bar_data_totals)+ scale_fill_brewer(palette = &quot;Blues&quot;) + labs(title=&quot;Bar chart example&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;Units&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;)+ theme(legend.title = element_blank())+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=10))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=10))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank())+ theme(panel.grid.major.y = element_blank()) + scale_y_continuous(expand=c(0,0),limits=c(0,1800))+ theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) 6.8 Histogram Aka. a bar chart for a continuous variable where the bars are touching. Useful to show distribution of time series or ordinal variables. c(&quot;0-20&quot;, &quot;20-40&quot;, &quot;40-60&quot;, &quot;60-80&quot;, &quot;80+&quot;) ## [1] &quot;0-20&quot; &quot;20-40&quot; &quot;40-60&quot; &quot;60-80&quot; &quot;80+&quot; #Create a data set hist_data &lt;- data.frame(X1=sample(0:100,100,rep=TRUE)) ggplot(hist_data)+ geom_histogram(aes(x=X1),binwidth=5,fill=&quot;blue&quot;,alpha=0.5) + geom_vline(xintercept=c(50,75,95),yintercept=0,linetype=&quot;longdash&quot;,col=&quot;orange&quot;) + labs(title=&quot;Histogram example&quot;, subtitle = &quot;Facet wraps are looking good&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.subtitle = element_text(margin=margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) 6.9 Ridge chart Handy when working with climate variables. Particularly useful at showing the difference in range of multiples series (e.g. temperature by month). # Import data url &lt;-&quot;https://raw.githubusercontent.com/charlescoverdale/ggridges/master/2019_MEL_max_temp_daily.xlsx&quot; MEL_temp_daily &lt;- openxlsx::read.xlsx(url) # Remove last 2 characters to just be left with the day number MEL_temp_daily$Day=substr(MEL_temp_daily$Day,1,nchar(MEL_temp_daily$Day)-2) # Make a wide format long using the gather function MEL_temp_daily &lt;- MEL_temp_daily %&gt;% gather(Month,Temp,Jan:Dec) MEL_temp_daily$Month&lt;-factor(MEL_temp_daily$Month,levels=c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;)) # Plot ggplot(MEL_temp_daily, aes(x = Temp, y = Month, fill = stat(x))) + geom_density_ridges_gradient(scale =2, size=0.3, rel_min_height = 0.01, gradient_lwd = 1.) + scale_y_discrete(limits = unique(rev(MEL_temp_daily$Month)))+ scale_fill_viridis_c(name = &quot;°C&quot;, option = &quot;C&quot;) + labs(title = &#39;Melbourne temperature profile&#39;, subtitle = &#39;Daily maximum temperature recorded in Melbourne in 2019&#39;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlab(&quot; &quot;)+ ylab(&quot; &quot;)+ theme_ridges(font_size = 13, grid = TRUE) 6.10 BBC style: Bar charts (categorical) # #Prepare data # bar_df &lt;- gapminder %&gt;% # filter(year == 2007 &amp; continent == &quot;Africa&quot;) %&gt;% # arrange(desc(lifeExp)) %&gt;% # head(5) # # #Make plot # bars &lt;- ggplot(bar_df, aes(x = country, y = lifeExp)) + # geom_bar(stat=&quot;identity&quot;, # position=&quot;identity&quot;, # fill=ifelse(bar_df$country == &quot;Mauritius&quot;, &quot;#1380A1&quot;, &quot;#dddddd&quot;)) + # geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + # bbc_style() + # labs(title=&quot;Reunion is highest&quot;, # subtitle = &quot;Highest African life expectancy, 2007&quot;) # # bars &lt;- bars + coord_flip() # bars &lt;- bars + coord_flip() + # theme(panel.grid.major.x = element_line(color=&quot;#cbcbcb&quot;), # panel.grid.major.y=element_blank()) # bars &lt;- bars + scale_y_continuous(limits=c(0,85), # breaks = seq(0, 80, by = 20), # labels = c(&quot;0&quot;,&quot;20&quot;, &quot;40&quot;, &quot;60&quot;, &quot;80 years&quot;)) # # labelled.bars &lt;- bars + # geom_label(aes(x = country, y = lifeExp, label = round(lifeExp, 0)), # hjust = 1, # vjust = 0.5, # colour = &quot;white&quot;, # fill = NA, # label.size = NA, # family=&quot;Helvetica&quot;, # size = 6) # # labelled.bars 6.11 BBC style: Dumbbell charts Dumbbell charts are handy instead of using clustered column charts with janky thinkcell labels and arrows to show the difference between the columns. Note it relies on having a 4 variable input (variable_name, value1, value2, and gap). The geom_dumbellfunction lives inside the ggalt package rather than the standard ggplot2. # #Prepare data # dumbbell_df &lt;- gapminder %&gt;% # filter(year == 1967 | year == 2007) %&gt;% # select(country, year, lifeExp) %&gt;% # spread(year, lifeExp) %&gt;% # mutate(gap = `2007` - `1967`) %&gt;% # arrange(desc(gap)) %&gt;% # head(10) # # #Make plot # ggplot(dumbbell_df, aes(x = `1967`, xend = `2007`, y = reorder(country, gap), group = country)) + geom_dumbbell(colour = &quot;#dddddd&quot;, # size = 3, # colour_x = &quot;#FAAB18&quot;, # colour_xend = &quot;#1380A1&quot;) + # bbc_style() + # labs(title=&quot;We&#39;re living longer&quot;, # subtitle=&quot;Biggest life expectancy rise, 1967-2007&quot;) 6.12 Facet wraps Handy rather than showing multiple lines on the same chart. Top tips: facet_wrap()dataframes need to be in long form in order to be manipulated easily. It also helps to add on separate columns for the start and end values (if you want to add data point labels). #Create a data set Year = c(&quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;) QLD = (c(500,300, 500, 600)) NSW = (c(200,400, 500, 700)) VIC = (c(300,400, 500, 600)) #Combine the columns into a single dataframe facet_data &lt;- (cbind(Year, QLD,NSW,VIC)) facet_data &lt;- as.data.frame(facet_data) #Change formats to integers facet_data$QLD = as.integer(facet_data$QLD) facet_data$NSW = as.integer(facet_data$NSW) facet_data$VIC = as.integer(facet_data$VIC) #Make the wide data long facet_data_long &lt;- pivot_longer(facet_data,!Year, names_to=&quot;State&quot;, values_to=&quot;Value&quot;) facet_data_long &lt;- facet_data_long %&gt;% dplyr::mutate(start_label = if_else(Year == min(Year), as.integer(Value), NA_integer_)) facet_data_long &lt;- facet_data_long %&gt;% dplyr::mutate(end_label = if_else(Year == max(Year), as.integer(Value), NA_integer_)) #Make the base line chart base_chart &lt;- ggplot() + geom_line(data=facet_data_long, aes(x = Year, y = Value, group = State, colour = State)) + geom_point(data=facet_data_long, aes(x = Year, y = Value, group = State, colour = State)) + ggrepel::geom_text_repel(data=facet_data_long, aes(x = Year, y = Value, label = end_label), color = &quot;black&quot;, nudge_y = -10,size=3) + ggrepel::geom_text_repel(data=facet_data_long, aes(x = Year, y = Value, label = start_label), color = &quot;black&quot;, nudge_y = 10,size=3) base_chart + scale_x_discrete( breaks = seq(2018, 2021, 1), labels = c(&quot;2018&quot;, &quot;19&quot;, &quot;20&quot;, &quot;21&quot;))+ facet_wrap(State ~ .) + #To control the grid arrangement, we can add in customer dimensions #ncol = 2, nrow=2) + labs(title=&quot;State by state comparison&quot;, subtitle = &quot;Facet wraps are looking good&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(strip.text.x = element_text(size = 9, face = &quot;bold&quot;)) + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.subtitle = element_text(margin=margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) 6.13 Pie chart These should be used sparingly but they are handy for showing proportions when the proportion of the whole is paramount (e.g. 45%) - rather than the proportion in relation to another data point (e.g. 16% one year vs 18% the next). # Create Data pie_data &lt;- data.frame( group=LETTERS[1:5], value=c(13,7,9,21,2)) # Compute the position of labels pie_data &lt;- pie_data %&gt;% arrange(desc(group)) %&gt;% mutate(proportion = value / sum(pie_data$value) *100) %&gt;% mutate(ypos = cumsum(proportion)- 0.5*proportion ) # Basic piechart ggplot(pie_data, aes(x=&quot;&quot;, y=proportion, fill=group)) + geom_bar(stat=&quot;identity&quot;)+ coord_polar(&quot;y&quot;, start=0) + theme_void()+ geom_text(aes(y = ypos, label = paste(round(proportion,digits=0),&quot;%&quot;, sep = &quot;&quot;),x=1.25), color = &quot;white&quot;, size=4) + scale_fill_brewer(palette=&quot;Set1&quot;)+ labs(title=&quot;Use pie charts sparingly&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme(legend.position = &quot;bottom&quot;)+ theme(legend.title = element_blank())+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(plot.subtitle = element_text(margin=margin(0,0,5,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) # ggsave(plot=last_plot(), # width=10, # height=10, # units=&quot;cm&quot;, # dpi = 600, # filename = &quot;/Users/charlescoverdale/Desktop/pietest.png&quot;) 6.14 Patchwork Patchwork is a nifty package for arranging plots and other graphic elements (text, tables etc) in different grid arrangements. The basic syntax is to use plot1 | plot2 for side by side charts, and plot1 / plot2for top and bottom charts. You can also combine these two functions for a grid of different size columns (e.g. plot3 / (plot1 | plot2) #Make some simply plots using the mtcars package p1 &lt;- ggplot(mtcars) + geom_point(aes(mpg, disp)) + ggtitle(&#39;Plot 1&#39;) p2 &lt;- ggplot(mtcars) + geom_boxplot(aes(gear, disp, group = gear)) + ggtitle(&#39;Plot 2&#39;) #Example of side by side charts p1 + p2 #Add in a table next to the plot p1 + gridExtra::tableGrob(mtcars[1:10, c(&#39;mpg&#39;, &#39;disp&#39;)]) 6.15 Saving to powerpoint Theres a bunch of ways to save ggplot graphics - but the way I find most useful is by exporting to pptx in a common charts directory. If you want to save as a png you can use the normal ggsave function - however it will not be editable (e.g. able to click and drag to rescale for a presentation). Therefore instead we can use the grattantheme package to easily save to an editable pptx graphic. Note: The code below has been commented out so that is will upload to bookdown.org without an error. #The classic save function to png # ggsave(plot = ggplot2::last_plot(), # width = 8, # height = 12, # dpi = 600, # filename = &quot;/Users/charlescoverdale/Desktop/test.png&quot;) #Using the grattantheme package to easily safe to powerpoint # grattan_save_pptx(p = ggplot2::last_plot(), # &quot;/Users/charlescoverdale/Desktop/test.pptx&quot;, # type = &quot;wholecolumn&quot;) 6.16 Automating chart creation Lets say we have a dataframe of multiple variables. We want to produce simple charts of the same style for each variable (including formatting and titles etc). Sure we can change the aes(x=)and aes(y=) variables in ggplot2 manually for each column - but this is time intensive especially for large data frames. Instead, we can write a function that will loop through the whole dataframe and produce the same format of chart. #Create a data set Year = c(&quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;) Variable1 = (c(500,300, 200, 400)) Variable2 = (c(200,400, 200, 700)) Variable3 = (c(300,500, 800, 1000)) #Combine the columns into a single dataframe bar_data_multiple &lt;- (cbind(Year, Variable1, Variable2, Variable3)) bar_data_multiple &lt;- as.data.frame(bar_data_multiple) #Change formats to integers bar_data_multiple$Variable1 = as.integer(bar_data_multiple$Variable1) bar_data_multiple$Variable2 = as.integer(bar_data_multiple$Variable2) bar_data_multiple$Variable3 = as.integer(bar_data_multiple$Variable3) #Define a function loop &lt;- function(chart_variable) { ggplot(bar_data_multiple, aes(x = Year, y = .data[[chart_variable]], label = .data[[chart_variable]]))+ geom_bar(stat=&#39;identity&#39;,fill=&quot;blue&quot;)+ geom_text(aes( label = scales::dollar(.data[[chart_variable]])), size = 5, col=&quot;white&quot;, fontface=&quot;bold&quot;, position = position_stack(vjust = 0.5))+ labs(title=paste(&quot;Company X: &quot;, chart_variable, &quot; (&quot;,head(Year,n=1), &quot; - &quot;, tail(Year,n=1), &quot;)&quot;, sep=&quot;&quot;), subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=12))+ theme(axis.text=element_text(size=12))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank())+ theme(panel.grid.major.y = element_blank()) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) } plots &lt;- purrr::map(colnames(bar_data_multiple)[colnames(bar_data_multiple) != &quot;Year&quot;], loop) plots #cowplot::plot_grid(plotlist = plots) "],["hypothesis-testing.html", "Chapter 7 Hypothesis testing 7.1 A quick refresher 7.2 T-testing our first hypothesis 7.3 Tailed tests 7.4 Correlation (and working with normal distributions) 7.5 Confidence intervals (mean) 7.6 Confidence intervals (model)", " Chapter 7 Hypothesis testing 7.1 A quick refresher Hypothesis testing is a way of validating if a claim about a population (e.g. a data set) is correct. Getting data on a whole population (e.g. everyone in Australia) is hard - so to validate a hypothesis, we use random samples from a population instead. The language when dealing with hypothesis testing is purposefully janky. When looking at the outputs of our hypothesis test, we consider p-values. Note: Theres lots wrong with p-values that we wont bother getting into right now. The long story short is if you make your null hypothesis ultra specific and only report when your p-value on your millionth iteration of a test is below 0.05 bad science is likely to get published and cited. What we need to know: A small p-value (typically &lt; 0.05) indicates strong evidence against the null hypothesis, so we reject it. A large p-value (&gt; 0.05) indicates weak evidence against the null hypothesis, so you fail to reject it. Lets load in some packages and get started. # Load in packages library(ggridges) library(ggplot2) library(forecast) library(ggrepel) library(viridis) library(readxl) library(hrbrthemes) library(dplyr) library(stringr) library(reshape) library(tidyr) library(lubridate) library(gapminder) library(ggalt) library(purrr) library(scales) library(purrr) library(aTSA) library(readrba) 7.2 T-testing our first hypothesis Well start simple. Lets create a random dataset - with the caveat that it will be normally distributed. By default the rnorm function will generate a dataset that has a mean of 0 and a standard deviation of 1. set.seed(40) dataset1 &lt;- data.frame(variable1=rnorm(1000,mean=0,sd=1)) ggplot()+ geom_histogram(aes(x=dataset1$variable1,y=..density..),binwidth=0.1,fill=&quot;blue&quot;,alpha=0.5) + stat_function(fun = dnorm, args = list(mean = mean(dataset1$variable1), sd = sd(dataset1$variable1)))+ geom_hline(yintercept = 0) + geom_vline(xintercept = 0, linetype=&quot;dotted&quot;,alpha=0.5)+ labs(title=&quot;Histogram for t-testing&quot;, caption = &quot;Data: Made from rnorm(1000)&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + scale_x_continuous(breaks = seq(-3, 3, by = 1))+ theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.subtitle = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) We know by default that the mean of dataset1 will be approximately zero but lets check anyway. # Find the mean of dataset1 mean(dataset1$variable1) Great, now lets run our first hypothesis test. Well use the t.test function. This is in the format of t.test(data, null_hypothesis). So quite simply, we can test the null hypothesis that the mean for dataset1$variable1 is 5). # Hypothesis test t.test(dataset1$variable1, mu = 5) We see the p-value here is tiny, meaning we reject the null hypothesis. That is to say, the mean for dataset1$variable1 is not 5. Going further, lets test the hypothesis that the mean is 0.01. # Hypothesis test t.test(dataset1$variable1, mu = -0.03, alternative=&quot;greater&quot;) We see here the p-value is greater than 0.05, leading us to fail to reject the null hypothesis. In a sentence, we cannot say that the mean of dataset1$variable1 is different to 0.01. 7.3 Tailed tests In the previous level we were cheating slightly, in that we didnt specify the tail for the test, or our confidence level. By default the t.test function assumes tests are two tailed, and the desired confidence level is 0.95 (i.e. 95%). However, in some cases we might want to have a hypothesis that says one variable is greater than or less than another variable (rather than just different from each other). This is where we use tails. # Hypothesis test t.test(dataset1$variable1, mu = 0.03, alternative = &quot;greater&quot;) 7.4 Correlation (and working with normal distributions) A correlation coefficient measures the direction and strength of the correlation between two variables. The tricky thing is - if variables arent normally distributed, none of our correlation theory works very well. In the example below, we see that miles per gallon is correlated with horsepower. Its a negative relationship, meaning the more horsepower in a car, the less miles per gallon the car exhibits. ggplot(mtcars)+ geom_point(aes(x = hp, y = mpg), col = &quot;blue&quot;, alpha=0.5)+ geom_smooth(aes(x = hp, y = mpg),method=&#39;lm&#39;)+ labs(title=&quot;Building a regression model&quot;, subtitle = &quot;Higher horsepower cars get less miles to the gallon&quot;, caption = &quot;Data: mtcars dataset&quot;, x=&quot;Horsepower&quot;, y=&quot;Miles per gallon&quot;) + theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 3, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)))+ theme(plot.subtitle = element_text(margin=ggplot2::margin(0,0,25,0))) + theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 3, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)))+ theme(plot.subtitle = element_text(margin=ggplot2::margin(0,0,25,0))) + theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4)) We can use the cor.test to tell us the correlation coefficient and the p-value of the correlation. We specify the method as pearson for the Pearson correlation coefficient. But we remember this method only works well if both our variables are normally distributed. Thats worth checking - lets plot a histogram for both hp and mpg. cor.test(mtcars$hp, mtcars$mpg, method=&quot;pearson&quot;) ggplot()+ geom_histogram(aes(x=mtcars$mpg,y=..density..),fill=&quot;blue&quot;,alpha=0.5) + stat_function(fun = dnorm, args = list(mean = mean(mtcars$mpg), sd = sd(mtcars$mpg)))+ geom_hline(yintercept = 0) + geom_vline(xintercept = mean(mtcars$mpg), linetype=&quot;dotted&quot;,alpha=0.5)+ labs(title=&quot;Histogram of mtcars$mpg&quot;, caption = &quot;Data: Made from mtcars&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) ggplot()+ geom_histogram(aes(x=mtcars$hp,y=..density..),fill=&quot;blue&quot;,alpha=0.5) + stat_function(fun = dnorm, args = list(mean = mean(mtcars$hp), sd = sd(mtcars$hp)))+ geom_hline(yintercept = 0) + geom_vline(xintercept = mean(mtcars$hp), linetype=&quot;dotted&quot;,alpha=0.5)+ labs(title=&quot;Histogram of mtcars$hp&quot;, caption = &quot;Data: Made from mtcars&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) Crikey they dont look very normal at all. Lets plot QQ plots of our variables and see whats going on. ggplot(mtcars, aes(sample = mpg)) + geom_qq()+ geom_qq_line()+ labs(title=&quot;QQ plot of mtcars$mpg&quot;, caption = &quot;Data: Made from mtcars&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) ggplot(mtcars, aes(sample = hp)) + geom_qq()+ geom_qq_line()+ labs(title=&quot;QQ plot of mtcars$hp&quot;, caption = &quot;Data: Made from mtcars&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) Hmm okay, both series are a bit all over the shop. Lets do a statistical test to confirm. The Shapiro-Wilks method is widely used for normality testing. The null hypothesis of this tests is that the sample distribution is normal. If the test is significant, the distribution is non-normal. shapiro.test(mtcars$mpg) shapiro.test(mtcars$hp) 7.5 Confidence intervals (mean) Firstly, we can calculate the confidence interval on a single variable. Essentially this measures the variance of the normal distribution, and gives us an idea of how clustered the values are to the mean. There are roughly 4 steps to do this: Calculate the mean Calculate the standard error of the mean Find the t-score that corresponds to the confidence level Calculate the margin of error and construct the confidence interval mpg.mean &lt;- mean(mtcars$mpg) print(mpg.mean) mpg.n &lt;- length(mtcars$mpg) mpg.sd &lt;- sd(mtcars$mpg) mpg.se &lt;- mpg.sd/sqrt(mpg.n) print(mpg.se) alpha = 0.05 degrees.freedom = mpg.n - 1 t.score = qt(p=alpha/2, df=degrees.freedom,lower.tail=F) print(t.score) mpg.error &lt;- t.score * mpg.se lower.bound &lt;- mpg.mean - mpg.error upper.bound &lt;- mpg.mean + mpg.error print(c(lower.bound,upper.bound)) For the lazy folks among us - theres also this quick and dirty way of doing it. # Calculate the mean and standard error mpg.model &lt;- lm(mpg ~ 1, mtcars) # Calculate the confidence interval confint(mpg.model, level=0.95) Great. Lets plot this interval on the distribution. ggplot()+ geom_histogram(aes(x=mtcars$mpg,y=..density..),binwidth=2,fill=&quot;blue&quot;,alpha=0.5) + stat_function(fun = dnorm, args = list(mean = mean(mtcars$mpg), sd = sd(mtcars$mpg)))+ geom_hline(yintercept = 0) + geom_vline(xintercept = mean(mtcars$mpg), linetype=&quot;dotted&quot;,alpha=0.5)+ geom_vline(xintercept = lower.bound,col=&quot;purple&quot;)+ geom_vline(xintercept = upper.bound, col=&quot;purple&quot;)+ labs(title=&quot;Histogram of mtcars$mpg&quot;, caption = &quot;Data: Made from mtcars&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) Two things we note here: First, the distribution doesnt look that normal. Second, geez that 95% confidence interval looks narrow as a result. Lets do the same analysis with an actual normal distribution and see what happens. set.seed(404) dataset2 &lt;- data.frame(variable1=rnorm(1000,mean=0,sd=1)) df2.mean &lt;- mean(dataset2$variable1) print(df2.mean) df2.n &lt;- length(dataset2$variable1) df2.sd &lt;- sd(dataset2$variable1) df2.se &lt;- mpg.sd/sqrt(mpg.n) print(df2.se) alpha = 0.05 degrees.freedom = df2.n - 1 t.score = qt(p=alpha/2, df=degrees.freedom,lower.tail=F) print(t.score) df2.error &lt;- t.score * df2.se lower.bound.df2 &lt;- df2.mean - df2.error upper.bound.df2 &lt;- df2.mean + df2.error print(c(lower.bound.df2,upper.bound.df2)) #Make a function to colour the tails upper_tail &lt;- function(x) { y &lt;- dnorm(x,mean=0,sd=1) y[x &lt; upper.bound.df2 | x &gt; 1000] &lt;- NA return(y) } lower_tail &lt;- function(x) { y &lt;- dnorm(x,mean=0,sd=1) y[x &lt; -1000 | x &gt; lower.bound.df2] &lt;- NA return(y) } #Plot the distributions ggplot()+ geom_histogram(aes(x=dataset2$variable1,y=..density..),binwidth=0.1,fill=&quot;blue&quot;,alpha=0.5) + stat_function(fun = dnorm, args = list(mean = mean(dataset2$variable1), sd = sd(dataset2$variable1)))+ geom_hline(yintercept = 0) + geom_vline(xintercept = mean(dataset2$variable1), linetype=&quot;dotted&quot;,alpha=0.5)+ geom_vline(xintercept = lower.bound.df2,col=&quot;purple&quot;)+ geom_vline(xintercept = upper.bound.df2, col=&quot;purple&quot;)+ stat_function(fun = upper_tail, geom = &quot;area&quot;, fill = &quot;grey&quot;,col=&quot;grey&quot;, alpha = 0.8) + stat_function(fun = lower_tail, geom = &quot;area&quot;, fill = &quot;grey&quot;,col=&quot;grey&quot;, alpha = 0.8) + labs(title=&quot;95% confidence interval&quot;, caption = &quot;Data: Made from rnorm(1000)&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + scale_x_continuous(breaks = seq(-3, 3, by = 1))+ theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) Great - weve got a more sensible looking plot, and greyed out the tails where our confidence interval excludes. We expect out observation to fall somewhere between the two purple lines (or more exactly between -2.1 and 2.1) 7.6 Confidence intervals (model) Secondly, we can calculate the confidence internal around a linear model. Somewhat similar to above, this process shows us how confident we can be about any single point in the linear estimate (e.g. if it has an enormous confidence interval attached to it the linear estimate of the value at that point is probably a bit dodgy). mtcars.lm &lt;- lm(mpg ~ hp, data = mtcars) summary(mtcars.lm) predict(mtcars.lm, newdata = mtcars, interval = &#39;confidence&#39;) Its great to have the raw data - but theres an even easier way of plotting the confidence interval on a chart: we use the geom_smooth()function. The syntax in this example is: geom_smooth(aes(x = hp, y = mpg), method='lm', level=0.95) ggplot(mtcars)+ geom_point(aes(x = hp, y = mpg), col = &quot;blue&quot;, alpha=0.5)+ geom_smooth(aes(x = hp, y = mpg),method=&#39;lm&#39;,level=0.95)+ labs(title=&quot;Building a regression model&quot;, subtitle = &quot;Higher horsepower cars get less miles to the gallon&quot;, caption = &quot;Data: mtcars dataset&quot;, x=&quot;Horsepower&quot;, y=&quot;Miles per gallon&quot;) + theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 3, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)))+ theme(plot.subtitle = element_text(margin=ggplot2::margin(0,0,25,0))) + theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 3, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)))+ theme(plot.subtitle = element_text(margin=ggplot2::margin(0,0,25,0))) + theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4)) For a sanity check, lets crank up the confidence level to 0.999 (meaning our interval should capture just about all the observations). We see the confidence interval band increases but not by that much. Why? Well remember how the data isnt a very good normal distribution? That means the confidence interval function wont be super accurate - especially at the extremes. "],["forecasting.html", "Chapter 8 Forecasting 8.1 Background 8.2 ARIMA models", " Chapter 8 Forecasting 8.1 Background So weve got a time series dataset but what is a reasonable forecast for how it might behave in the future? Sure we can do a confidence interval (as we learned in the previous chapter) - but what about forecasting for multiple periods into the future. Thats where we need to build some models. # Load in packages library(ggridges) library(ggplot2) library(forecast) library(ggrepel) library(viridis) library(readxl) library(hrbrthemes) library(dplyr) library(stringr) library(reshape) library(tidyr) library(lubridate) library(gapminder) library(ggalt) library(purrr) library(scales) library(purrr) library(aTSA) library(readrba) Well start with some pre-loaded time series data. The ggplot2 includes a data set called economics that contains US economic indicators from the 1960s to 2015. econ_data &lt;- economics %&gt;% dplyr::select(c(&quot;date&quot;, &quot;uempmed&quot;)) econ_data &lt;- econ_data %&gt;% dplyr::filter((date &gt;= as.Date(&quot;1970-01-01&quot;) &amp; date &lt;= as.Date(&quot;1999-12-31&quot;))) As a side note: We can also get Australian unemployment rate data using the readrba function. aus_unemp_rate &lt;- read_rba(series_id = &quot;GLFSURSA&quot;) head(aus_unemp_rate) Lets plot the data to see what we are working with. ggplot(econ_data)+ geom_point(aes(x = date, y = uempmed), col = &quot;grey&quot;, alpha=0.5)+ geom_smooth(aes(x = date, y = uempmed), col = &quot;blue&quot;)+ labs(title=&quot;Unemployment rate&quot;, caption = &quot;Data: ggplot2::economics&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 3, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)))+ theme(plot.title = element_text(margin=ggplot2::margin(0,0,25,0))) + theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4)) 8.2 ARIMA models AutoRegressive Integrated Moving Average (ARIMA) models are a handy tool to have in the toolbox. An auto regressive model is one where Yt depends on its own lags. A moving average (MA only) model is one where Yt depends only on the lagged forecast errors. We combine these together (technically we integrate them) and get ARIMA. First order of business, we may need to difference our series to make it stationary. Lets check if it is stationary using the augmented Dickey-Fuller test. The null hypothesis assumes that the series is non-stationary. A series is said to be stationary when its mean, variance, and autocovariance dont change much over time. # Test for stationarity aTSA::adf.test(econ_data$uempmed) ## Augmented Dickey-Fuller Test ## alternative: stationary ## ## Type 1: no drift no trend ## lag ADF p.value ## [1,] 0 -0.448 0.515 ## [2,] 1 -0.279 0.564 ## [3,] 2 -0.252 0.571 ## [4,] 3 -0.218 0.581 ## [5,] 4 -0.321 0.552 ## [6,] 5 -0.399 0.529 ## Type 2: with drift no trend ## lag ADF p.value ## [1,] 0 -3.05 0.0337 ## [2,] 1 -2.54 0.1158 ## [3,] 2 -2.43 0.1572 ## [4,] 3 -2.58 0.0986 ## [5,] 4 -2.68 0.0826 ## [6,] 5 -2.82 0.0597 ## Type 3: with drift and trend ## lag ADF p.value ## [1,] 0 -2.94 0.179 ## [2,] 1 -2.33 0.438 ## [3,] 2 -2.20 0.491 ## [4,] 3 -2.33 0.439 ## [5,] 4 -2.49 0.368 ## [6,] 5 -2.69 0.285 ## ---- ## Note: in fact, p.value = 0.01 means p.value &lt;= 0.01 # See the auto correlation acf(econ_data$uempmed) # Identify patial auto correlation Pacf(econ_data$uempmed) # Take the first differences of the series econ_data &lt;- econ_data %&gt;% mutate(diff = uempmed-lag(uempmed)) ggplot(econ_data)+ geom_point(aes(x = date, y = diff), col = &quot;grey&quot;, alpha=0.5)+ geom_smooth(aes(x = date, y = diff), col = &quot;blue&quot;)+ labs(title=&quot;1st difference (Unemployment rate)&quot;, caption = &quot;Data: ggplot2::economics&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 3, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)))+ theme(plot.title = element_text(margin=ggplot2::margin(0,0,25,0))) + theme(axis.line.x = element_line(colour =&quot;black&quot;,size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;,size=0.4)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). ARIMA_model = forecast::auto.arima(econ_data$uempmed) ARIMA_model summary(ARIMA_model) checkresiduals(ARIMA_model) # Forecast for the next 10 time units ARIMA_forecast &lt;- forecast::forecast(ARIMA_model, newdata=econ_data$uempmed,h = 36,level=c(95)) # Plot forecasts plot((ARIMA_forecast)) "],["text-mining.html", "Chapter 9 Text-mining 9.1 Power with words 9.2 Frequency analysis 9.3 Sentiment analysis", " Chapter 9 Text-mining 9.1 Power with words Numbers are great but words literally tell a story. Analysing text (e.g. books, tweets, survey responses) in a quantitative format is naturally challenging - however theres a few tricks which can simplify the process. This chapter outlines the process for inputting text data, and running some simple analysis. The notes and code loosely follow the fabulous book Text Mining with R by Julia Silge and David Robinson. First up, lets load some packages. library(ggplot2) library(dplyr) library(tidyverse) library(tidytext) library(textdata) 9.2 Frequency analysis Theres a online depository called Project Gutenberg which catalogue texts that have lost their copyright (mostly because it expires over time). These can be called with the R package called gutenbergr It just so happens that The Bible is on this list. Lets check out the most frequent words. library(gutenbergr) bible &lt;- gutenberg_download(30) bible_tidy &lt;- bible %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) #Find the most common words bible_tidy %&gt;% count(word, sort=TRUE) ## # A tibble: 12,595 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 lord 7830 ## 2 thou 5474 ## 3 thy 4600 ## 4 god 4446 ## 5 ye 3982 ## 6 thee 3827 ## 7 001 2783 ## 8 002 2721 ## 9 israel 2565 ## 10 003 2560 ## # ... with 12,585 more rows Somewhat unsurprisingly - lord wins it by a country mile. 9.3 Sentiment analysis Just like a frequency analysis, we can do a vibe analysis (i.e. sentiment of a text) using a clever thesaurus matching technique. In the tidytext package are lexicons which include the general sentiment of words (e.g. the emotion you can use to describe that word). Lets see the count of words most associated with joy in the bible. #Download sentiment list nrcjoy &lt;- get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment == &quot;joy&quot;) #Join bible words with sentiment list bible_tidy %&gt;% inner_join(nrcjoy) %&gt;% count(word, sort=TRUE) ## # A tibble: 258 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 god 4446 ## 2 art 494 ## 3 peace 429 ## 4 found 402 ## 5 glory 402 ## 6 daughter 324 ## 7 pray 313 ## 8 love 310 ## 9 blessed 302 ## 10 mighty 284 ## # ... with 248 more rows "],["economic-indicators.html", "Chapter 10 Economic indicators 10.1 Overview 10.2 Gross Domestic Product 10.3 Unemployment rate 10.4 Inflation (CPI) 10.5 Wage Price Index 10.6 RBA cash rate 10.7 AUD exchange rate", " Chapter 10 Economic indicators 10.1 Overview Australia has exceptional financial and economic institutions. Three of these institutions release periodic data useful for economic analysis: Australian Bureau of Statistics Reserve Bank of Australia Treasury As usual, there are catches. Most of this data is in inconsistent formats (the reasons for which continue to baffle me). Whats more, its currently not possible to ping databases or APIs for access to this data it is mainly accessed through spreadsheets. The scripts below run through some of the main ways to import, clean, and analyse Australian macroeconomic data in R. Some of the key packages well use are readabs and readrba. To get started, lets install and load packages. #Loads the required required packages pacman::p_load(ggmap, ggplot2, dplyr, tmaptools, RCurl, jsonlite, tidyverse, leaflet, writexl, readr, readxl, readabs, readrba,lubridate, zoo, scales) 10.2 Gross Domestic Product To get GDP data from the ABS, well use the read_abs function from the readrba package. #For simplicity, we keep the download function seperate to the analysis all_gdp &lt;- read_abs(&quot;5206.0&quot;) #Select the seasonally adjusted data and filter for data and value columns gdp_level &lt;- all_gdp %&gt;% filter(series == &quot;Gross domestic product: Chain volume measures ;&quot;, !is.na(value)) %&gt;% filter(series_type ==&quot;Seasonally Adjusted&quot;) %&gt;% dplyr::select(date,value) %&gt;% dplyr::rename(quarterly_output=value) gdp_level &lt;- gdp_level %&gt;% mutate(quarterly_growth_rate = ((quarterly_output / lag(quarterly_output,1)-1))*100) %&gt;% mutate(annual_gdp = rollapply(quarterly_output, 4, sum, na.rm=TRUE, fill = NA, align = &quot;right&quot;)) %&gt;% mutate(annual_gdp_trillions=annual_gdp/1000000)%&gt;% mutate(annual_growth_rate = ((annual_gdp / lag(annual_gdp, 4) - 1))*100)%&gt;% mutate(Quarter_of_year = lubridate::quarter(date, with_year = FALSE, fiscal_start = 1)) #Set a baseline value gdp_level$baseline_value &lt;- gdp_level$quarterly_output[ which(gdp_level$date ==&quot;2022-03-01&quot;)] gdp_level &lt;- gdp_level %&gt;% mutate(baseline_comparison = (quarterly_output/baseline_value)*100) tail(gdp_level) ## # A tibble: 6 x 9 ## date quarterly_output quarterly_growth_rate annual_gdp annual_gdp_trill~ ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-12-01 501644 3.28 1960142 1.96 ## 2 2021-03-01 510590 1.78 1967236 1.97 ## 3 2021-06-01 514784 0.821 2012745 2.01 ## 4 2021-09-01 505413 -1.82 2032431 2.03 ## 5 2021-12-01 523725 3.62 2054512 2.05 ## 6 2022-03-01 527676 0.754 2071598 2.07 ## # ... with 4 more variables: annual_growth_rate &lt;dbl&gt;, Quarter_of_year &lt;int&gt;, ## # baseline_value &lt;dbl&gt;, baseline_comparison &lt;dbl&gt; Now we can plot the GDP data for Australia. plot_gdp &lt;- ggplot(data=gdp_level)+ geom_line((aes(x=date, y=annual_gdp_trillions)), col=&quot;blue&quot;) + labs(title = &quot;Australian GDP ($AUD)&quot;, subtitle = &quot;Annualised figures&quot;, caption = &quot;Data: Australian Bureau of Statistics&quot;, y = &quot;&quot;, x = &quot; &quot;)+ scale_y_continuous(breaks = c(0,0.5,1.0,1.5,2.0,2.5), labels = label_number(suffix = &quot; trillion&quot;))+ scale_x_date(date_breaks = &quot;10 years&quot;, date_labels=&quot;%Y&quot;)+ theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;, size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -45)))+ theme(axis.line.x = element_line(colour =&quot;black&quot;, size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;, size=0.4)) plot_gdp 10.3 Unemployment rate Download the data #Download the time sreies all_unemployment &lt;- read_abs(&quot;6202.0&quot;) Clean and analyse the data unemployment_rate &lt;- all_unemployment %&gt;% filter(series == &quot;Unemployment rate ; Persons ;&quot;,!is.na(value)) %&gt;% filter(table_title==&quot;Table 1. Labour force status by Sex, Australia - Trend, Seasonally adjusted and Original&quot;) %&gt;% filter(series_type ==&quot;Seasonally Adjusted&quot;) %&gt;% mutate(mean_unemployment_rate=mean(value)) %&gt;% mutate(percentile_25=quantile(value,0.25))%&gt;% mutate(percentile_75=quantile(value,0.75)) %&gt;% dplyr::select(date,value,mean_unemployment_rate,percentile_25,percentile_75) tail(unemployment_rate) ## # A tibble: 6 x 5 ## date value mean_unemployment_rate percentile_25 percentile_75 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2021-11-01 4.60 6.74 5.47 8.01 ## 2 2021-12-01 4.16 6.74 5.47 8.01 ## 3 2022-01-01 4.18 6.74 5.47 8.01 ## 4 2022-02-01 4.02 6.74 5.47 8.01 ## 5 2022-03-01 3.93 6.74 5.47 8.01 ## 6 2022-04-01 3.85 6.74 5.47 8.01 Plot the data plot_unemployment_rate &lt;- ggplot(data=unemployment_rate)+ geom_line(aes(x = date, y = value), col = &quot;blue&quot;) + labs(title = &quot;Unemployment rate&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Australian Bureau of Statistics&quot;, y = &quot;Unemployment rate (%)&quot;, x = &quot; &quot;)+ scale_y_continuous(labels = scales::comma)+ scale_x_date(date_breaks = &quot;10 years&quot;, date_labels=&quot;%Y&quot;)+ theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;, size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -15)))+ theme(axis.line.x = element_line(colour =&quot;black&quot;, size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;, size=0.4)) plot_unemployment_rate 10.4 Inflation (CPI) Download the data all_CPI &lt;- read_abs(&quot;6401.0&quot;) Clean and analyse the data Australia_CPI &lt;- all_CPI %&gt;% filter(series == &quot;Percentage Change from Corresponding Quarter of Previous Year ; All groups CPI ; Australia ;&quot;,!is.na(value)) %&gt;% mutate(mean_CPI=mean(value)) %&gt;% mutate(percentile_25=quantile(value,0.25))%&gt;% mutate(percentile_75=quantile(value,0.75)) %&gt;% dplyr::select(date, value,mean_CPI,percentile_25,percentile_75) tail(Australia_CPI) ## # A tibble: 6 x 5 ## date value mean_CPI percentile_25 percentile_75 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-12-01 0.9 4.94 1.9 7.3 ## 2 2021-03-01 1.1 4.94 1.9 7.3 ## 3 2021-06-01 3.8 4.94 1.9 7.3 ## 4 2021-09-01 3 4.94 1.9 7.3 ## 5 2021-12-01 3.5 4.94 1.9 7.3 ## 6 2022-03-01 5.1 4.94 1.9 7.3 #Can add in the below line to filter #filter(date&gt;&quot;2010-01-01&quot;) %&gt;% Plot the data plot_CPI &lt;- ggplot(data=Australia_CPI %&gt;% filter(date&gt;(as.Date(&quot;2000-01-01&quot;))))+ geom_rect(aes(xmin=as.Date(&quot;2000-01-01&quot;), xmax=as.Date(&quot;2023-03-01&quot;), ymin=2, ymax=3), alpha=0.01, fill=&quot;grey&quot;)+ geom_line(aes(x = date, y = value), col = &quot;blue&quot;) + scale_x_continuous(expand=c(0,0))+ labs(title = &quot;Inflation (as measured by the CPI)&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Australian Bureau of Statistics&quot;, y = &quot;(%)&quot;, x = &quot; &quot;)+ scale_y_continuous(labels = scales::comma)+ scale_x_date(date_breaks = &quot;5 years&quot;, date_labels=&quot;%Y&quot;)+ theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;, size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -15)))+ theme(axis.line.x = element_line(colour =&quot;black&quot;, size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;, size=0.4)) plot_CPI Plot a histogram of the data plot_CPI_hist &lt;- ggplot(Australia_CPI, aes(x=value)) + geom_histogram(aes(y=..density..), colour=&quot;black&quot;, fill=&quot;lightblue&quot;)+ geom_density(alpha=.5, fill=&quot;grey&quot;,colour=&quot;darkblue&quot;)+ scale_x_continuous(expand=c(0,0))+ labs(title = &quot;Consumer Price Index: Histogram&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Australian Bureau of Statistics&quot;, y = &quot;(%)&quot;, x = &quot; &quot;)+ theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;, size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 20, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -2)))+ theme(axis.line.x = element_line(colour =&quot;black&quot;, size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;, size=0.4)) plot_CPI_hist 10.5 Wage Price Index Download the data all_wpi &lt;- read_abs(&quot;6345.0&quot;) Clean and analyse the data Australia_WPI &lt;- all_wpi %&gt;% filter(series == &quot;Percentage Change From Corresponding Quarter of Previous Year ; Australia ; Total hourly rates of pay excluding bonuses ; Private and Public ; All industries ;&quot;, !is.na(value)) %&gt;% filter(series_type==&quot;Seasonally Adjusted&quot;) %&gt;% mutate(mean_WPI=mean(value)) %&gt;% dplyr::select(date, value,mean_WPI) tail(Australia_WPI) ## # A tibble: 6 x 3 ## date value mean_WPI ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-12-01 1.4 3.06 ## 2 2021-03-01 1.5 3.06 ## 3 2021-06-01 1.7 3.06 ## 4 2021-09-01 2.2 3.06 ## 5 2021-12-01 2.3 3.06 ## 6 2022-03-01 2.4 3.06 Plot the data plot_WPI &lt;- ggplot(data=Australia_WPI)+ geom_line(aes(x = date, y = value), col = &quot;blue&quot;) + labs(title = &quot;Wage Price Index&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Australian Bureau of Statistics&quot;, y = &quot;(%)&quot;, x = &quot; &quot;)+ scale_y_continuous(labels = scales::comma)+ scale_x_date(date_breaks = &quot;5 years&quot;, date_labels=&quot;%Y&quot;)+ theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;, size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -15)))+ theme(axis.line.x = element_line(colour =&quot;black&quot;, size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;, size=0.4)) plot_WPI 10.6 RBA cash rate Download the data cash_rate_all&lt;- readrba::read_rba(table_no = &quot;F13&quot;) head(cash_rate_all) ## # A tibble: 6 x 11 ## date series value frequency series_type units source pub_date ## &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 1990-01-31 Australia Targ~ 17 Monthly Original Per ~ RBA 2022-06-02 ## 2 1990-02-28 Australia Targ~ 16.5 Monthly Original Per ~ RBA 2022-06-02 ## 3 1990-03-31 Australia Targ~ 16.5 Monthly Original Per ~ RBA 2022-06-02 ## 4 1990-04-30 Australia Targ~ 15 Monthly Original Per ~ RBA 2022-06-02 ## 5 1990-05-31 Australia Targ~ 15 Monthly Original Per ~ RBA 2022-06-02 ## 6 1990-06-30 Australia Targ~ 15 Monthly Original Per ~ RBA 2022-06-02 ## # ... with 3 more variables: series_id &lt;chr&gt;, description &lt;chr&gt;, ## # table_title &lt;chr&gt; Clean and analyse the data cash_rate_Australia &lt;- cash_rate_all %&gt;% filter(series==&quot;Australia Target Cash Rate&quot;) %&gt;% dplyr::select(date, value) tail(cash_rate_Australia) ## # A tibble: 6 x 2 ## date value ## &lt;date&gt; &lt;dbl&gt; ## 1 2021-12-31 0.1 ## 2 2022-01-31 0.1 ## 3 2022-02-28 0.1 ## 4 2022-03-31 0.1 ## 5 2022-04-30 0.1 ## 6 2022-05-31 0.35 Plot the data plot_cash_rate &lt;- ggplot(data=cash_rate_Australia)+ geom_line(aes(x = date, y = value), col = &quot;blue&quot;) + labs(title = &quot;RBA cash rate&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Read RBA&quot;, y = &quot; &quot;, x = &quot; &quot;)+ scale_y_continuous(labels = scales::comma)+ scale_x_date(date_breaks = &quot;3 years&quot;, date_labels=&quot;%Y&quot;)+ theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;, size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -15)))+ theme(axis.line.x = element_line(colour =&quot;black&quot;, size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;, size=0.4)) plot_cash_rate 10.7 AUD exchange rate God knows why - but but there are super quirky names for the official exchange rate tables Download the data exchange_rate_all&lt;- readrba::read_rba(table_no = (c(&quot;ex_daily_8386&quot;, &quot;ex_daily_8790&quot;, &quot;ex_daily_9194&quot;, &quot;ex_daily_9598&quot;, &quot;ex_daily_9902&quot;, &quot;ex_daily_0306&quot;, &quot;ex_daily_0709&quot;, &quot;ex_daily_1013&quot;, &quot;ex_daily_1417&quot;, &quot;ex_daily_18cur&quot;)), cur_hist = &quot;historical&quot;) Clean and analyse the data exchange_rate_AUD &lt;- exchange_rate_all %&gt;% filter(series==&quot;A$1=USD&quot;) %&gt;% dplyr::select(date, value) tail(exchange_rate_AUD) ## # A tibble: 6 x 2 ## date value ## &lt;date&gt; &lt;dbl&gt; ## 1 2022-06-06 0.720 ## 2 2022-06-07 0.718 ## 3 2022-06-08 0.720 ## 4 2022-06-09 0.718 ## 5 2022-06-10 0.712 ## 6 2022-06-14 0.697 Plot the data plot_exchange_rate_AUD &lt;- ggplot(data=exchange_rate_AUD)+ geom_line(aes(x = date, y = value), col = &quot;blue&quot;) + labs(title = &quot;AUD exchange rate&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Reserve Bank of Australia&quot;, y = &quot; &quot;, x = &quot; &quot;)+ scale_y_continuous(labels = scales::comma)+ scale_x_date(date_breaks = &quot;3 years&quot;, date_labels=&quot;%Y&quot;)+ theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;, size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -15)))+ theme(axis.line.x = element_line(colour =&quot;black&quot;, size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;, size=0.4)) plot_exchange_rate_AUD "],["geocoding-in-r.html", "Chapter 11 Geocoding in R 11.1 About geocoding 11.2 Introducing the dataset 11.3 Data cleaning 11.4 Running the geocode 11.5 From geocode to shapefile", " Chapter 11 Geocoding in R 11.1 About geocoding Geocoding allows us to find coordinates for a data point (or vice versa). For instance, if we have the street address of an asset/object we can use geocoding to find the latitude and longitude. Similarly, if we have the lat and lon, we can find the street address. This allows us to turn boring address data into a spatial data point. This makes geocoding immensely useful when trying to find the catchments around a particular asset/object. Its also handy in visualising location based data on a map. To find the lat and long for an address - we have to find someone with a massive database of addresses and coordinates. Unsurprisingly, Google (through their Google Maps team) has this data. We can ping Googles data base through an API (they charge a freemium model) to find the data we need. Further reading Breaking Down Geocoding in R: A Complete Guide | by Oleksandr Titorchuk | Towards Data Science 11.2 Introducing the dataset Approved speed camera locations in Victoria, Australia are publicly available through the government website. We can download the dataset as a spreadsheet. Note: this dataset does not have street numbers - only street names. This makes intuitive sense (as most speed cameras arent placed outside residential houses, but rather along main roads). However, it will cause some niche issues in the geocoding process. 11.3 Data cleaning First, we start by installing and loading up some packages. #Loads the required required packages pacman::p_load(ggmap, tmaptools, RCurl, jsonlite, tidyverse, leaflet, writexl, readr, readxl, sf, mapview, webshot,rgdal, tinytex) #Make sure the HTML works webshot::install_phantomjs() Next, we import the data as a CSV. We can see that the street and suburb names are in different columns. To run a geocode, we want a single field of address data. Lets combine the street name and the suburb name into a single field. We know all these addresses are in Australia, so we can add the word Australia to the end of each entry in the newly created address field to help the geocoder find the location. We think all these addresses are in the state of Victoria however lets not at the state in at this stage (we can see why later). #Read in spreadsheet url &lt;- &#39;https://raw.githubusercontent.com/charlescoverdale/bookdata/main/mobile-camera-locations-june-2022.csv&#39; #Note: We need to import as a csv rather than xlsx for url functionality camera_address &lt;- read.csv(url, header=TRUE) #Fix the column labels camera_address &lt;- janitor::row_to_names(camera_address,1) #Convert the suburb column to title case camera_address$SUBURB &lt;- str_to_title(camera_address$SUBURB) #Concatenate the two fields into a single address field camera_address$ADDRESS &lt;- paste(camera_address$LOCATION, camera_address$SUBURB, sep=&quot;, &quot;) #Add in Australia to the address field just to idiot proof the df camera_address$ADDRESS &lt;- paste(camera_address$ADDRESS, &quot;, Australia&quot;, sep=&quot;&quot;) #Preview the data head(camera_address) ## LOCATION SUBURB Reason Code Audit Date ## 2 Abey Road Cobblebank BCD May-22 ## 3 Adam Street Golden Square ACD Apr-22 ## 4 Agar Road Coronet Bay BC T ## 5 Airport Drive Melbourne Airport ABCD Apr-22 ## 6 Aitken Boulevard Roxburgh Park ABC Apr-22 ## 7 Aitken Boulevard Craigieburn ABCD Apr-22 ## ADDRESS ## 2 Abey Road, Cobblebank, Australia ## 3 Adam Street, Golden Square, Australia ## 4 Agar Road, Coronet Bay, Australia ## 5 Airport Drive, Melbourne Airport, Australia ## 6 Aitken Boulevard, Roxburgh Park, Australia ## 7 Aitken Boulevard, Craigieburn, Australia 11.4 Running the geocode Now that we have a useful address field, were ready to run the geocode. We can geocode the lats and lons using the Google API through ggmap package. We must register a API key (by creating an account in the google developer suite). You can ping the API 2,500 times a day. Lucky for us this dataset is only 1,800 rows long! Uncomment the chunk below (using Ctrl+Shift+C) and enter your unique key from google to run the code. # #Input the google API key # register_google(key = &quot;PASTE YOUR UNIQUE KEY HERE&quot;) # # #Run the geocode function from ggmap package # camera_ggmap &lt;- ggmap::geocode(location = camera_address$ADDRESS, # output = &quot;more&quot;, # source = &quot;google&quot;) # # #We&#39;ll bind the newly created address columns to the original df # camera_ggmap &lt;- cbind(camera_address, camera_ggmap) # # #Print the results # head(camera_ggmap) # # #Write the data to a df # readr::write_csv(camera_ggmap,&quot;C:/Data/camera_geocoded.csv&quot;) Well load up the output this chunk generates and continue. url &lt;- &#39;https://raw.githubusercontent.com/charlescoverdale/bookdata/main/camera_geocoded.csv&#39; #Note: We need to import as a csv rather than xlsx for url functionality camera_ggmap &lt;- read.csv(url, header=TRUE) 11.5 From geocode to shapefile The raw dataset our geocode produced looks good! Although it could probably use some cleaning. Lets rename this dataset so we dont loose the original df (and therefore have to run the google query again). This is simply a best practice step to build in some redundancy. camera_geocoded &lt;- camera_ggmap #Rename the API generated address field camera_geocoded_clean &lt;- camera_geocoded %&gt;% dplyr::rename(address_long=address) #Select only the columns we need camera_geocoded_clean &lt;- camera_geocoded_clean %&gt;% dplyr::select(&quot;LOCATION&quot;, &quot;SUBURB&quot;, &quot;ADDRESS&quot;, &quot;lon&quot;, &quot;lat&quot;, &quot;address_long&quot; ) #Make all the column names the same format camera_geocoded_clean &lt;- janitor::clean_names(camera_geocoded_clean) #We still need to convert address_long to title case camera_geocoded_clean$address_long &lt;- str_to_title(camera_geocoded_clean$address_long) If we want to go one step further, we can create spatial points from this list of coordinates. This is a good step for eyeballing the data. We see most of it is in Victoria (as expected!) but it has picked up a couple of points in Sydney and WA. These are worth investigating seperately for correction or exclusion. #Convert data frame to sf object camera_points &lt;- sf::st_as_sf(x = camera_geocoded_clean, coords = c(&quot;lon&quot;, &quot;lat&quot;), crs = &quot;+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0&quot;) #Plot an interactive map mapview(camera_points) We can export these points as a shapefile using the rgdal package. #Write the points to a shapefile for use in QGIS #sf::st_write(camera_points, &quot;C:/Data/camera_points.shp&quot;) Lets now have a look at our edge case datapoints. Theres a couple of ways to do this but one of the simplest is to extract the postcode as a separate field. We can then simply sort by postcodes that do not start with the number 3. camera_points$postcode &lt;- str_sub(camera_points$address_long, start= -16, end= -12) camera_points$postcode &lt;- as.numeric(camera_points$postcode) outliers &lt;- camera_points %&gt;% filter (postcode &lt;3000 | postcode &gt;= 4000) print(outliers) ## Simple feature collection with 7 features and 5 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 115.9555 ymin: -33.90941 xmax: 152.7034 ymax: -25.53849 ## CRS: +proj=longlat ## +datum=WGS84 ## +ellps=WGS84 ## +towgs84=0,0,0 ## location suburb ## 1 Epping Road Epping ## 2 High Street Epping ## 3 Kensington Road Kensington ## 4 Maryborough Road Ascot ## 5 Maryborough-Ballarat Road Talbot ## 6 Mooroopna-Murchison Road Murchison ## 7 Plumpton Road Plumpton ## address ## 1 Epping Road, Epping, Australia ## 2 High Street, Epping, Australia ## 3 Kensington Road, Kensington, Australia ## 4 Maryborough Road, Ascot, Australia ## 5 Maryborough-Ballarat Road, Talbot, Australia ## 6 Mooroopna-Murchison Road, Murchison, Australia ## 7 Plumpton Road, Plumpton, Australia ## address_long geometry ## 1 Epping Rd, Epping Nsw 2121, Australia POINT (151.0906 -33.77076) ## 2 High St, Epping Nsw 2121, Australia POINT (151.0836 -33.77611) ## 3 Kensington Rd, Kensington Nsw 2033, Australia POINT (151.2211 -33.90941) ## 4 Maryborough Qld 4650, Australia POINT (152.7034 -25.53849) ## 5 Maryborough Qld 4650, Australia POINT (152.7034 -25.53849) ## 6 Murchison Wa 6630, Australia POINT (115.9555 -26.89259) ## 7 Plumpton Rd, Plumpton Nsw 2761, Australia POINT (150.8439 -33.74823) ## postcode ## 1 2121 ## 2 2121 ## 3 2033 ## 4 4650 ## 5 4650 ## 6 6630 ## 7 2761 Easy enough. We see theres 7 rows that dont have a postcode starting in a 3000 postcode. Four of these are in NSW, two in QLD, and one in WA. It looks like they are real (e.g. the streets and suburbs do exist in that state) so for now lets just exclude them from our dataset. camera_points_vic &lt;- camera_points %&gt;% filter (postcode&gt;&quot;3000&quot; &amp; postcode&lt;&quot;4000&quot;) #Plot an interactive map mapview(camera_points_vic) #Write the points to a shapefile for use in QGIS #sf::st_write(camera_points_vic, &quot;C:/Data/camera_points_vic.shp&quot;) There we go! A clean, geocoded dataset of speed camera locations in Victoria. "],["drive-time-analysis.html", "Chapter 12 Drive time analysis 12.1 Background on drive times 12.2 Method 1: OSRM package", " Chapter 12 Drive time analysis 12.1 Background on drive times A drive time describes how far you can drive (i.e in a car on a public road) in a certain amount of time. Running a drive time analysis is useful to identify demographic catchments around a point (e.g. a school, hospital, road, or precinct. This can assist in defining the catchment of users for a particular infrastructure asset. These polygons can then be overlayed with ABS census data (e.g. SA2) and spliced in with census variables (age, income, housing, SES status, etc). Theres various companies that own drive time data. Most of these are map makers (e.g. google, ESRI, and Tom Tom). To get started, lets install and load packages. #Loads the required required packages pacman::p_load(ggmap, tmaptools, RCurl, jsonlite, tidyverse, leaflet, writexl, readr, readxl, sf, mapview, rgdal, webshot,osrm) 12.2 Method 1: OSRM package Useful links for further reading: Source 1, Source 2 The OSRM package (Github) pulls from OpenStreetMap to find travel times based on location. The downside is that the polygons it generates are pretty chunky i.e. it doesnt take into account major roads and streets as the key tributaries/arteries of a city area. We can get around this a bit by dialing up the res (i.e. the resolution) in the osrmIsochrone function but its only a partial solution. #Create a dataframe with the lat and long locations &lt;- tibble::tribble(~place, ~lon, ~lat, &quot;Melbourne&quot;, 144.9631, -37.8136) #Run it through the osrm package iso &lt;- osrmIsochrone(loc = c(locations$lon, locations$lat), breaks = seq(from = 0, to = 30, by = 5), res=50) #iso@data$drive_times &lt;- factor(paste(iso@data$min, &quot;to&quot;, iso@data$max, &quot;mins&quot;)) #factpal &lt;- colorFactor(&quot;RdYlBu&quot;, iso@data$drive_times) leaflet() %&gt;% setView(mean(locations$lon), mean(locations$lat), zoom = 7) %&gt;% addProviderTiles(&quot;CartoDB.Positron&quot;, group=&quot;Greyscale&quot;) %&gt;% addMarkers(lng = locations$lon, locations$lat) %&gt;% addPolygons(fill=TRUE, stroke=TRUE, color = &quot;black&quot;, #fillColor = ~factpal(iso@data$drive_times), weight=0.5, fillOpacity=0.2, data = iso, #popup = iso@data$drive_times, group = &quot;Drive Time&quot;) #%&gt;% # addLegend(&quot;bottomright&quot;, pal = factpal, # values = iso@data$drive_time, # title = &quot;Drive Time&quot;) We can export these polygons as shapefiles for use in QGIS or other spatial programs. #Write the points to a shapefile for use in QGIS #sf::st_write(iso, &quot;C:/Data/iso_polygons.shp&quot;) "]]
